{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPif1b/MraXsQ5vQm3z/oX0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jinholeetopology/ml/blob/master/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRe8F7_i-c8U"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTE_ReD2-gCw"
      },
      "source": [
        "{\n",
        " \"cells\": [\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": 8,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [\n",
        "    {\n",
        "     \"data\": {\n",
        "      \"text/html\": [\n",
        "       \"<div>\\n\",\n",
        "       \"<style scoped>\\n\",\n",
        "       \"    .dataframe tbody tr th:only-of-type {\\n\",\n",
        "       \"        vertical-align: middle;\\n\",\n",
        "       \"    }\\n\",\n",
        "       \"\\n\",\n",
        "       \"    .dataframe tbody tr th {\\n\",\n",
        "       \"        vertical-align: top;\\n\",\n",
        "       \"    }\\n\",\n",
        "       \"\\n\",\n",
        "       \"    .dataframe thead th {\\n\",\n",
        "       \"        text-align: right;\\n\",\n",
        "       \"    }\\n\",\n",
        "       \"</style>\\n\",\n",
        "       \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n",
        "       \"  <thead>\\n\",\n",
        "       \"    <tr style=\\\"text-align: right;\\\">\\n\",\n",
        "       \"      <th></th>\\n\",\n",
        "       \"      <th>시가</th>\\n\",\n",
        "       \"      <th>고가</th>\\n\",\n",
        "       \"      <th>저가</th>\\n\",\n",
        "       \"      <th>현재가</th>\\n\",\n",
        "       \"      <th>거래량</th>\\n\",\n",
        "       \"    </tr>\\n\",\n",
        "       \"    <tr>\\n\",\n",
        "       \"      <th>일자</th>\\n\",\n",
        "       \"      <th></th>\\n\",\n",
        "       \"      <th></th>\\n\",\n",
        "       \"      <th></th>\\n\",\n",
        "       \"      <th></th>\\n\",\n",
        "       \"      <th></th>\\n\",\n",
        "       \"    </tr>\\n\",\n",
        "       \"  </thead>\\n\",\n",
        "       \"  <tbody>\\n\",\n",
        "       \"    <tr>\\n\",\n",
        "       \"      <th>2018-05-11</th>\\n\",\n",
        "       \"      <td>318.03</td>\\n\",\n",
        "       \"      <td>319.70</td>\\n\",\n",
        "       \"      <td>317.86</td>\\n\",\n",
        "       \"      <td>318.51</td>\\n\",\n",
        "       \"      <td>112,400</td>\\n\",\n",
        "       \"    </tr>\\n\",\n",
        "       \"    <tr>\\n\",\n",
        "       \"      <th>2018-05-10</th>\\n\",\n",
        "       \"      <td>316.51</td>\\n\",\n",
        "       \"      <td>317.34</td>\\n\",\n",
        "       \"      <td>315.11</td>\\n\",\n",
        "       \"      <td>317.31</td>\\n\",\n",
        "       \"      <td>120,900</td>\\n\",\n",
        "       \"    </tr>\\n\",\n",
        "       \"    <tr>\\n\",\n",
        "       \"      <th>2018-05-09</th>\\n\",\n",
        "       \"      <td>315.93</td>\\n\",\n",
        "       \"      <td>316.08</td>\\n\",\n",
        "       \"      <td>312.60</td>\\n\",\n",
        "       \"      <td>314.42</td>\\n\",\n",
        "       \"      <td>123,610</td>\\n\",\n",
        "       \"    </tr>\\n\",\n",
        "       \"    <tr>\\n\",\n",
        "       \"      <th>2018-05-08</th>\\n\",\n",
        "       \"      <td>317.79</td>\\n\",\n",
        "       \"      <td>319.53</td>\\n\",\n",
        "       \"      <td>314.95</td>\\n\",\n",
        "       \"      <td>315.87</td>\\n\",\n",
        "       \"      <td>141,252</td>\\n\",\n",
        "       \"    </tr>\\n\",\n",
        "       \"    <tr>\\n\",\n",
        "       \"      <th>2018-05-04</th>\\n\",\n",
        "       \"      <td>320.56</td>\\n\",\n",
        "       \"      <td>321.00</td>\\n\",\n",
        "       \"      <td>316.75</td>\\n\",\n",
        "       \"      <td>316.75</td>\\n\",\n",
        "       \"      <td>173,911</td>\\n\",\n",
        "       \"    </tr>\\n\",\n",
        "       \"  </tbody>\\n\",\n",
        "       \"</table>\\n\",\n",
        "       \"</div>\"\n",
        "      ],\n",
        "      \"text/plain\": [\n",
        "       \"                시가      고가      저가     현재가      거래량\\n\",\n",
        "       \"일자                                                 \\n\",\n",
        "       \"2018-05-11  318.03  319.70  317.86  318.51  112,400\\n\",\n",
        "       \"2018-05-10  316.51  317.34  315.11  317.31  120,900\\n\",\n",
        "       \"2018-05-09  315.93  316.08  312.60  314.42  123,610\\n\",\n",
        "       \"2018-05-08  317.79  319.53  314.95  315.87  141,252\\n\",\n",
        "       \"2018-05-04  320.56  321.00  316.75  316.75  173,911\"\n",
        "      ]\n",
        "     },\n",
        "     \"execution_count\": 8,\n",
        "     \"metadata\": {},\n",
        "     \"output_type\": \"execute_result\"\n",
        "    }\n",
        "   ],\n",
        "   \"source\": [\n",
        "    \"# sam01_save.py\\n\",\n",
        "    \"\\n\",\n",
        "    \"import numpy as np\\n\",\n",
        "    \"import pandas as pd\\n\",\n",
        "    \"\\n\",\n",
        "    \"df1 = pd.read_csv(\\\".\\\\kospi200\\\\data\\\\kospi200.csv\\\", index_col=0, encoding='cp949')\\n\",\n",
        "    \"df1.tail()   \"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": 6,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [\n",
        "    {\n",
        "     \"name\": \"stdout\",\n",
        "     \"output_type\": \"stream\",\n",
        "     \"text\": [\n",
        "      \"(426, 5)\\n\"\n",
        "     ]\n",
        "    }\n",
        "   ],\n",
        "   \"source\": [\n",
        "    \"print(df1.shape)\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": 11,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [\n",
        "    {\n",
        "     \"data\": {\n",
        "      \"text/html\": [\n",
        "       \"<div>\\n\",\n",
        "       \"<style scoped>\\n\",\n",
        "       \"    .dataframe tbody tr th:only-of-type {\\n\",\n",
        "       \"        vertical-align: middle;\\n\",\n",
        "       \"    }\\n\",\n",
        "       \"\\n\",\n",
        "       \"    .dataframe tbody tr th {\\n\",\n",
        "       \"        vertical-align: top;\\n\",\n",
        "       \"    }\\n\",\n",
        "       \"\\n\",\n",
        "       \"    .dataframe thead th {\\n\",\n",
        "       \"        text-align: right;\\n\",\n",
        "       \"    }\\n\",\n",
        "       \"</style>\\n\",\n",
        "       \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n",
        "       \"  <thead>\\n\",\n",
        "       \"    <tr style=\\\"text-align: right;\\\">\\n\",\n",
        "       \"      <th></th>\\n\",\n",
        "       \"      <th>시가</th>\\n\",\n",
        "       \"      <th>고가</th>\\n\",\n",
        "       \"      <th>저가</th>\\n\",\n",
        "       \"      <th>종가</th>\\n\",\n",
        "       \"      <th>거래량</th>\\n\",\n",
        "       \"    </tr>\\n\",\n",
        "       \"    <tr>\\n\",\n",
        "       \"      <th>일자</th>\\n\",\n",
        "       \"      <th></th>\\n\",\n",
        "       \"      <th></th>\\n\",\n",
        "       \"      <th></th>\\n\",\n",
        "       \"      <th></th>\\n\",\n",
        "       \"      <th></th>\\n\",\n",
        "       \"    </tr>\\n\",\n",
        "       \"  </thead>\\n\",\n",
        "       \"  <tbody>\\n\",\n",
        "       \"    <tr>\\n\",\n",
        "       \"      <th>2018-05-11</th>\\n\",\n",
        "       \"      <td>52,000</td>\\n\",\n",
        "       \"      <td>52,200</td>\\n\",\n",
        "       \"      <td>51,200</td>\\n\",\n",
        "       \"      <td>51,300</td>\\n\",\n",
        "       \"      <td>10,314,997</td>\\n\",\n",
        "       \"    </tr>\\n\",\n",
        "       \"    <tr>\\n\",\n",
        "       \"      <th>2018-05-10</th>\\n\",\n",
        "       \"      <td>51,700</td>\\n\",\n",
        "       \"      <td>51,700</td>\\n\",\n",
        "       \"      <td>50,600</td>\\n\",\n",
        "       \"      <td>51,600</td>\\n\",\n",
        "       \"      <td>13,905,263</td>\\n\",\n",
        "       \"    </tr>\\n\",\n",
        "       \"    <tr>\\n\",\n",
        "       \"      <th>2018-05-09</th>\\n\",\n",
        "       \"      <td>52,600</td>\\n\",\n",
        "       \"      <td>52,800</td>\\n\",\n",
        "       \"      <td>50,900</td>\\n\",\n",
        "       \"      <td>50,900</td>\\n\",\n",
        "       \"      <td>16,128,305</td>\\n\",\n",
        "       \"    </tr>\\n\",\n",
        "       \"    <tr>\\n\",\n",
        "       \"      <th>2018-05-08</th>\\n\",\n",
        "       \"      <td>52,600</td>\\n\",\n",
        "       \"      <td>53,200</td>\\n\",\n",
        "       \"      <td>51,900</td>\\n\",\n",
        "       \"      <td>52,600</td>\\n\",\n",
        "       \"      <td>23,104,720</td>\\n\",\n",
        "       \"    </tr>\\n\",\n",
        "       \"    <tr>\\n\",\n",
        "       \"      <th>2018-05-04</th>\\n\",\n",
        "       \"      <td>53,000</td>\\n\",\n",
        "       \"      <td>53,900</td>\\n\",\n",
        "       \"      <td>51,800</td>\\n\",\n",
        "       \"      <td>51,900</td>\\n\",\n",
        "       \"      <td>39,565,391</td>\\n\",\n",
        "       \"    </tr>\\n\",\n",
        "       \"  </tbody>\\n\",\n",
        "       \"</table>\\n\",\n",
        "       \"</div>\"\n",
        "      ],\n",
        "      \"text/plain\": [\n",
        "       \"                시가      고가      저가      종가         거래량\\n\",\n",
        "       \"일자                                                    \\n\",\n",
        "       \"2018-05-11  52,000  52,200  51,200  51,300  10,314,997\\n\",\n",
        "       \"2018-05-10  51,700  51,700  50,600  51,600  13,905,263\\n\",\n",
        "       \"2018-05-09  52,600  52,800  50,900  50,900  16,128,305\\n\",\n",
        "       \"2018-05-08  52,600  53,200  51,900  52,600  23,104,720\\n\",\n",
        "       \"2018-05-04  53,000  53,900  51,800  51,900  39,565,391\"\n",
        "      ]\n",
        "     },\n",
        "     \"execution_count\": 11,\n",
        "     \"metadata\": {},\n",
        "     \"output_type\": \"execute_result\"\n",
        "    }\n",
        "   ],\n",
        "   \"source\": [\n",
        "    \"df2 = pd.read_csv(\\\".\\\\kospi200\\\\data\\\\samsung.csv\\\", index_col=0, encoding='cp949')\\n\",\n",
        "    \"df2.tail()\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": 12,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [\n",
        "    {\n",
        "     \"name\": \"stdout\",\n",
        "     \"output_type\": \"stream\",\n",
        "     \"text\": [\n",
        "      \"(426, 5)\\n\"\n",
        "     ]\n",
        "    }\n",
        "   ],\n",
        "   \"source\": [\n",
        "    \"print(df2.shape)\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": 13,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [\n",
        "    {\n",
        "     \"name\": \"stdout\",\n",
        "     \"output_type\": \"stream\",\n",
        "     \"text\": [\n",
        "      \"<class 'pandas.core.frame.DataFrame'>\\n\",\n",
        "      \"Index: 426 entries, 2020-01-31 to 2018-05-04\\n\",\n",
        "      \"Data columns (total 5 columns):\\n\",\n",
        "      \"시가     426 non-null float64\\n\",\n",
        "      \"고가     426 non-null float64\\n\",\n",
        "      \"저가     426 non-null float64\\n\",\n",
        "      \"현재가    426 non-null float64\\n\",\n",
        "      \"거래량    426 non-null object\\n\",\n",
        "      \"dtypes: float64(4), object(1)\\n\",\n",
        "      \"memory usage: 20.0+ KB\\n\"\n",
        "     ]\n",
        "    }\n",
        "   ],\n",
        "   \"source\": [\n",
        "    \"df1.info()\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": 15,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"df1['거래량'] = df1['거래량'].str.replace(',','').astype(int)\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": 16,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [\n",
        "    {\n",
        "     \"name\": \"stdout\",\n",
        "     \"output_type\": \"stream\",\n",
        "     \"text\": [\n",
        "      \"<class 'pandas.core.frame.DataFrame'>\\n\",\n",
        "      \"Index: 426 entries, 2020-01-31 to 2018-05-04\\n\",\n",
        "      \"Data columns (total 5 columns):\\n\",\n",
        "      \"시가     426 non-null object\\n\",\n",
        "      \"고가     426 non-null object\\n\",\n",
        "      \"저가     426 non-null object\\n\",\n",
        "      \"종가     426 non-null object\\n\",\n",
        "      \"거래량    426 non-null object\\n\",\n",
        "      \"dtypes: object(5)\\n\",\n",
        "      \"memory usage: 20.0+ KB\\n\"\n",
        "     ]\n",
        "    }\n",
        "   ],\n",
        "   \"source\": [\n",
        "    \"df2.info()\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": 17,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [\n",
        "    {\n",
        "     \"data\": {\n",
        "      \"text/plain\": [\n",
        "       \"Index(['시가', '고가', '저가', '종가', '거래량'], dtype='object')\"\n",
        "      ]\n",
        "     },\n",
        "     \"execution_count\": 17,\n",
        "     \"metadata\": {},\n",
        "     \"output_type\": \"execute_result\"\n",
        "    }\n",
        "   ],\n",
        "   \"source\": [\n",
        "    \"df2.columns\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": 18,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"for i in df2.columns:\\n\",\n",
        "    \"    df2[i] = df2[i].str.replace(',','').astype(int)\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": 19,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [\n",
        "    {\n",
        "     \"name\": \"stdout\",\n",
        "     \"output_type\": \"stream\",\n",
        "     \"text\": [\n",
        "      \"<class 'pandas.core.frame.DataFrame'>\\n\",\n",
        "      \"Index: 426 entries, 2020-01-31 to 2018-05-04\\n\",\n",
        "      \"Data columns (total 5 columns):\\n\",\n",
        "      \"시가     426 non-null int32\\n\",\n",
        "      \"고가     426 non-null int32\\n\",\n",
        "      \"저가     426 non-null int32\\n\",\n",
        "      \"종가     426 non-null int32\\n\",\n",
        "      \"거래량    426 non-null int32\\n\",\n",
        "      \"dtypes: int32(5)\\n\",\n",
        "      \"memory usage: 11.6+ KB\\n\"\n",
        "     ]\n",
        "    }\n",
        "   ],\n",
        "   \"source\": [\n",
        "    \"df2.info()\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": 20,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [\n",
        "    {\n",
        "     \"data\": {\n",
        "      \"text/html\": [\n",
        "       \"<div>\\n\",\n",
        "       \"<style scoped>\\n\",\n",
        "       \"    .dataframe tbody tr th:only-of-type {\\n\",\n",
        "       \"        vertical-align: middle;\\n\",\n",
        "       \"    }\\n\",\n",
        "       \"\\n\",\n",
        "       \"    .dataframe tbody tr th {\\n\",\n",
        "       \"        vertical-align: top;\\n\",\n",
        "       \"    }\\n\",\n",
        "       \"\\n\",\n",
        "       \"    .dataframe thead th {\\n\",\n",
        "       \"        text-align: right;\\n\",\n",
        "       \"    }\\n\",\n",
        "       \"</style>\\n\",\n",
        "       \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n",
        "       \"  <thead>\\n\",\n",
        "       \"    <tr style=\\\"text-align: right;\\\">\\n\",\n",
        "       \"      <th></th>\\n\",\n",
        "       \"      <th>시가</th>\\n\",\n",
        "       \"      <th>고가</th>\\n\",\n",
        "       \"      <th>저가</th>\\n\",\n",
        "       \"      <th>현재가</th>\\n\",\n",
        "       \"      <th>거래량</th>\\n\",\n",
        "       \"    </tr>\\n\",\n",
        "       \"    <tr>\\n\",\n",
        "       \"      <th>일자</th>\\n\",\n",
        "       \"      <th></th>\\n\",\n",
        "       \"      <th></th>\\n\",\n",
        "       \"      <th></th>\\n\",\n",
        "       \"      <th></th>\\n\",\n",
        "       \"      <th></th>\\n\",\n",
        "       \"    </tr>\\n\",\n",
        "       \"  </thead>\\n\",\n",
        "       \"  <tbody>\\n\",\n",
        "       \"    <tr>\\n\",\n",
        "       \"      <th>2018-05-04</th>\\n\",\n",
        "       \"      <td>320.56</td>\\n\",\n",
        "       \"      <td>321.00</td>\\n\",\n",
        "       \"      <td>316.75</td>\\n\",\n",
        "       \"      <td>316.75</td>\\n\",\n",
        "       \"      <td>173911</td>\\n\",\n",
        "       \"    </tr>\\n\",\n",
        "       \"    <tr>\\n\",\n",
        "       \"      <th>2018-05-08</th>\\n\",\n",
        "       \"      <td>317.79</td>\\n\",\n",
        "       \"      <td>319.53</td>\\n\",\n",
        "       \"      <td>314.95</td>\\n\",\n",
        "       \"      <td>315.87</td>\\n\",\n",
        "       \"      <td>141252</td>\\n\",\n",
        "       \"    </tr>\\n\",\n",
        "       \"    <tr>\\n\",\n",
        "       \"      <th>2018-05-09</th>\\n\",\n",
        "       \"      <td>315.93</td>\\n\",\n",
        "       \"      <td>316.08</td>\\n\",\n",
        "       \"      <td>312.60</td>\\n\",\n",
        "       \"      <td>314.42</td>\\n\",\n",
        "       \"      <td>123610</td>\\n\",\n",
        "       \"    </tr>\\n\",\n",
        "       \"    <tr>\\n\",\n",
        "       \"      <th>2018-05-10</th>\\n\",\n",
        "       \"      <td>316.51</td>\\n\",\n",
        "       \"      <td>317.34</td>\\n\",\n",
        "       \"      <td>315.11</td>\\n\",\n",
        "       \"      <td>317.31</td>\\n\",\n",
        "       \"      <td>120900</td>\\n\",\n",
        "       \"    </tr>\\n\",\n",
        "       \"    <tr>\\n\",\n",
        "       \"      <th>2018-05-11</th>\\n\",\n",
        "       \"      <td>318.03</td>\\n\",\n",
        "       \"      <td>319.70</td>\\n\",\n",
        "       \"      <td>317.86</td>\\n\",\n",
        "       \"      <td>318.51</td>\\n\",\n",
        "       \"      <td>112400</td>\\n\",\n",
        "       \"    </tr>\\n\",\n",
        "       \"    <tr>\\n\",\n",
        "       \"      <th>...</th>\\n\",\n",
        "       \"      <td>...</td>\\n\",\n",
        "       \"      <td>...</td>\\n\",\n",
        "       \"      <td>...</td>\\n\",\n",
        "       \"      <td>...</td>\\n\",\n",
        "       \"      <td>...</td>\\n\",\n",
        "       \"    </tr>\\n\",\n",
        "       \"    <tr>\\n\",\n",
        "       \"      <th>2020-01-23</th>\\n\",\n",
        "       \"      <td>303.77</td>\\n\",\n",
        "       \"      <td>304.72</td>\\n\",\n",
        "       \"      <td>301.71</td>\\n\",\n",
        "       \"      <td>302.33</td>\\n\",\n",
        "       \"      <td>86908</td>\\n\",\n",
        "       \"    </tr>\\n\",\n",
        "       \"    <tr>\\n\",\n",
        "       \"      <th>2020-01-28</th>\\n\",\n",
        "       \"      <td>294.98</td>\\n\",\n",
        "       \"      <td>296.30</td>\\n\",\n",
        "       \"      <td>291.30</td>\\n\",\n",
        "       \"      <td>292.77</td>\\n\",\n",
        "       \"      <td>130172</td>\\n\",\n",
        "       \"    </tr>\\n\",\n",
        "       \"    <tr>\\n\",\n",
        "       \"      <th>2020-01-29</th>\\n\",\n",
        "       \"      <td>294.38</td>\\n\",\n",
        "       \"      <td>295.67</td>\\n\",\n",
        "       \"      <td>292.45</td>\\n\",\n",
        "       \"      <td>293.98</td>\\n\",\n",
        "       \"      <td>85731</td>\\n\",\n",
        "       \"    </tr>\\n\",\n",
        "       \"    <tr>\\n\",\n",
        "       \"      <th>2020-01-30</th>\\n\",\n",
        "       \"      <td>293.27</td>\\n\",\n",
        "       \"      <td>294.11</td>\\n\",\n",
        "       \"      <td>287.09</td>\\n\",\n",
        "       \"      <td>288.37</td>\\n\",\n",
        "       \"      <td>101535</td>\\n\",\n",
        "       \"    </tr>\\n\",\n",
        "       \"    <tr>\\n\",\n",
        "       \"      <th>2020-01-31</th>\\n\",\n",
        "       \"      <td>290.24</td>\\n\",\n",
        "       \"      <td>291.47</td>\\n\",\n",
        "       \"      <td>284.53</td>\\n\",\n",
        "       \"      <td>284.53</td>\\n\",\n",
        "       \"      <td>101455</td>\\n\",\n",
        "       \"    </tr>\\n\",\n",
        "       \"  </tbody>\\n\",\n",
        "       \"</table>\\n\",\n",
        "       \"<p>426 rows × 5 columns</p>\\n\",\n",
        "       \"</div>\"\n",
        "      ],\n",
        "      \"text/plain\": [\n",
        "       \"                시가      고가      저가     현재가     거래량\\n\",\n",
        "       \"일자                                                \\n\",\n",
        "       \"2018-05-04  320.56  321.00  316.75  316.75  173911\\n\",\n",
        "       \"2018-05-08  317.79  319.53  314.95  315.87  141252\\n\",\n",
        "       \"2018-05-09  315.93  316.08  312.60  314.42  123610\\n\",\n",
        "       \"2018-05-10  316.51  317.34  315.11  317.31  120900\\n\",\n",
        "       \"2018-05-11  318.03  319.70  317.86  318.51  112400\\n\",\n",
        "       \"...            ...     ...     ...     ...     ...\\n\",\n",
        "       \"2020-01-23  303.77  304.72  301.71  302.33   86908\\n\",\n",
        "       \"2020-01-28  294.98  296.30  291.30  292.77  130172\\n\",\n",
        "       \"2020-01-29  294.38  295.67  292.45  293.98   85731\\n\",\n",
        "       \"2020-01-30  293.27  294.11  287.09  288.37  101535\\n\",\n",
        "       \"2020-01-31  290.24  291.47  284.53  284.53  101455\\n\",\n",
        "       \"\\n\",\n",
        "       \"[426 rows x 5 columns]\"\n",
        "      ]\n",
        "     },\n",
        "     \"execution_count\": 20,\n",
        "     \"metadata\": {},\n",
        "     \"output_type\": \"execute_result\"\n",
        "    }\n",
        "   ],\n",
        "   \"source\": [\n",
        "    \"df1.sort_values(by = '일자')\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": 24,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [\n",
        "    {\n",
        "     \"data\": {\n",
        "      \"text/plain\": [\n",
        "       \"426\"\n",
        "      ]\n",
        "     },\n",
        "     \"execution_count\": 24,\n",
        "     \"metadata\": {},\n",
        "     \"output_type\": \"execute_result\"\n",
        "    }\n",
        "   ],\n",
        "   \"source\": [\n",
        "    \"# df1 과 df2 의 index 가 같은지 확인! ok\\n\",\n",
        "    \"\\n\",\n",
        "    \"sum(df1.index == df2.index)\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": 29,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [\n",
        "    {\n",
        "     \"data\": {\n",
        "      \"text/plain\": [\n",
        "       \"array([   290.24,    291.47,    284.53,    284.53, 101455.  ])\"\n",
        "      ]\n",
        "     },\n",
        "     \"execution_count\": 29,\n",
        "     \"metadata\": {},\n",
        "     \"output_type\": \"execute_result\"\n",
        "    }\n",
        "   ],\n",
        "   \"source\": [\n",
        "    \"df1.values[0]\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": 27,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [\n",
        "    {\n",
        "     \"data\": {\n",
        "      \"text/plain\": [\n",
        "       \"(426, 5)\"\n",
        "      ]\n",
        "     },\n",
        "     \"execution_count\": 27,\n",
        "     \"metadata\": {},\n",
        "     \"output_type\": \"execute_result\"\n",
        "    }\n",
        "   ],\n",
        "   \"source\": [\n",
        "    \"df1.values.shape\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": 60,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"df1 = df1.sort_values(['일자'], ascending=[True])\\n\",\n",
        "    \"df2 = df2.sort_values(['일자'], ascending=[True])\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": 61,\n",
        "   \"metadata\": {\n",
        "    \"scrolled\": true\n",
        "   },\n",
        "   \"outputs\": [\n",
        "    {\n",
        "     \"name\": \"stdout\",\n",
        "     \"output_type\": \"stream\",\n",
        "     \"text\": [\n",
        "      \"<class 'numpy.ndarray'> <class 'numpy.ndarray'>\\n\",\n",
        "      \"(426, 5) (426, 5)\\n\"\n",
        "     ]\n",
        "    }\n",
        "   ],\n",
        "   \"source\": [\n",
        "    \"df1 = df1.values\\n\",\n",
        "    \"df2 = df2.values\\n\",\n",
        "    \"\\n\",\n",
        "    \"print(type(df1), type(df2))\\n\",\n",
        "    \"print(df1.shape, df2.shape)\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": null,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"np.save('./kospi200/data/kospi200.npy', arr=df1)\\n\",\n",
        "    \"np.save('./kospi200/data/samsung.npy', arr=df2)\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": 37,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [\n",
        "    {\n",
        "     \"name\": \"stdout\",\n",
        "     \"output_type\": \"stream\",\n",
        "     \"text\": [\n",
        "      \"[[   53000    53900    51800    51900 39565391]\\n\",\n",
        "      \" [   52600    53200    51900    52600 23104720]\\n\",\n",
        "      \" [   52600    52800    50900    50900 16128305]\\n\",\n",
        "      \" [   51700    51700    50600    51600 13905263]\\n\",\n",
        "      \" [   52000    52200    51200    51300 10314997]] \\n\",\n",
        "      \" [50100]\\n\",\n",
        "      \"(421, 5, 5)\\n\",\n",
        "      \"(421, 1)\\n\"\n",
        "     ]\n",
        "    }\n",
        "   ],\n",
        "   \"source\": [\n",
        "    \"# sam02.py\\n\",\n",
        "    \"\\n\",\n",
        "    \"kospi200 = np.load('./kospi200/data/kospi200.npy')\\n\",\n",
        "    \"samsung = np.load('./kospi200/data/samsung.npy')\\n\",\n",
        "    \"\\n\",\n",
        "    \"def split_xy5(dataset, time_steps, y_column):\\n\",\n",
        "    \"    x, y = list(), list()\\n\",\n",
        "    \"    for i in range(len(dataset)):\\n\",\n",
        "    \"        x_end_number = i + time_steps\\n\",\n",
        "    \"        y_end_number = x_end_number + y_column # 수정\\n\",\n",
        "    \"\\n\",\n",
        "    \"        if y_end_number > len(dataset):  # 수정\\n\",\n",
        "    \"            break\\n\",\n",
        "    \"            \\n\",\n",
        "    \"        tmp_x = dataset[i:x_end_number, :]  # 수정\\n\",\n",
        "    \"        tmp_y = dataset[x_end_number:y_end_number, 3]    # 수정\\n\",\n",
        "    \"        #print(tmp_x,tmp_y)\\n\",\n",
        "    \"        x.append(tmp_x)\\n\",\n",
        "    \"        y.append(tmp_y)\\n\",\n",
        "    \"        \\n\",\n",
        "    \"    return np.array(x), np.array(y)\\n\",\n",
        "    \"\\n\",\n",
        "    \"x, y = split_xy5(samsung, 5, 1) \\n\",\n",
        "    \"\\n\",\n",
        "    \"print(x[0,:], \\\"\\\\n\\\", y[0])\\n\",\n",
        "    \"print(x.shape)\\n\",\n",
        "    \"print(y.shape)\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": 38,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [\n",
        "    {\n",
        "     \"name\": \"stdout\",\n",
        "     \"output_type\": \"stream\",\n",
        "     \"text\": [\n",
        "      \"(294, 5, 5)\\n\",\n",
        "      \"(127, 5, 5)\\n\",\n",
        "      \"(294, 1)\\n\",\n",
        "      \"(127, 1)\\n\",\n",
        "      \"(294, 25)\\n\",\n",
        "      \"(127, 25)\\n\"\n",
        "     ]\n",
        "    }\n",
        "   ],\n",
        "   \"source\": [\n",
        "    \"# 데이터 셋 나누기\\n\",\n",
        "    \"from sklearn.model_selection import train_test_split\\n\",\n",
        "    \"# from sklearn.model_selection import cross_val_score\\n\",\n",
        "    \"x_train, x_test, y_train, y_test = train_test_split(\\n\",\n",
        "    \"    x, y, random_state=1, test_size = 0.3)\\n\",\n",
        "    \"\\n\",\n",
        "    \"print(x_train.shape)\\n\",\n",
        "    \"print(x_test.shape)\\n\",\n",
        "    \"print(y_train.shape)\\n\",\n",
        "    \"print(y_test.shape)\\n\",\n",
        "    \"\\n\",\n",
        "    \"x_train = np.reshape(x_train,\\n\",\n",
        "    \"    (x_train.shape[0], x_train.shape[1] * x_train.shape[2]))\\n\",\n",
        "    \"x_test = np.reshape(x_test,\\n\",\n",
        "    \"    (x_test.shape[0], x_test.shape[1] * x_test.shape[2]))\\n\",\n",
        "    \"\\n\",\n",
        "    \"print(x_train.shape)\\n\",\n",
        "    \"print(x_test.shape)\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": 39,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [\n",
        "    {\n",
        "     \"name\": \"stdout\",\n",
        "     \"output_type\": \"stream\",\n",
        "     \"text\": [\n",
        "      \"[ 1.13392563  1.07633086  1.02393741  0.93187013 -0.028819    1.05994459\\n\",\n",
        "      \"  1.04756614  1.06686328  0.94933344  1.53138774  0.79093879  0.80213886\\n\",\n",
        "      \"  0.84540479  0.81507608  0.10811605  0.73712199  0.68893234  0.71073997\\n\",\n",
        "      \"  0.69657856  0.37360361  0.85647275  0.80021877  0.80910994  0.69442952\\n\",\n",
        "      \" -0.16973555]\\n\"\n",
        "     ]\n",
        "    }\n",
        "   ],\n",
        "   \"source\": [\n",
        "    \"#### 데이터 전처리 #####\\n\",\n",
        "    \"\\n\",\n",
        "    \"from sklearn.preprocessing import StandardScaler\\n\",\n",
        "    \"\\n\",\n",
        "    \"scaler = StandardScaler()\\n\",\n",
        "    \"scaler.fit(x_train)\\n\",\n",
        "    \"x_train_scaled = scaler.transform(x_train)\\n\",\n",
        "    \"x_test_scaled = scaler.transform(x_test)\\n\",\n",
        "    \"\\n\",\n",
        "    \"print(x_train_scaled[0, :])\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": 41,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [\n",
        "    {\n",
        "     \"name\": \"stdout\",\n",
        "     \"output_type\": \"stream\",\n",
        "     \"text\": [\n",
        "      \"Train on 235 samples, validate on 59 samples\\n\",\n",
        "      \"Epoch 1/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 1764983011.0960 - mse: 1764982400.0000 - val_loss: 710128824.2867 - val_mse: 710128704.0000\\n\",\n",
        "      \"Epoch 2/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 917us/step - loss: 907789123.7004 - mse: 907789312.0000 - val_loss: 671254445.0763 - val_mse: 671254592.0000\\n\",\n",
        "      \"Epoch 3/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 912us/step - loss: 850949342.2632 - mse: 850949184.0000 - val_loss: 650040241.4439 - val_mse: 650040256.0000\\n\",\n",
        "      \"Epoch 4/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 904us/step - loss: 774011698.1230 - mse: 774011648.0000 - val_loss: 560592681.4809 - val_mse: 560592704.0000\\n\",\n",
        "      \"Epoch 5/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 857us/step - loss: 679587083.1287 - mse: 679587008.0000 - val_loss: 476983010.8316 - val_mse: 476983040.0000\\n\",\n",
        "      \"Epoch 6/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 823us/step - loss: 497605927.2107 - mse: 497605856.0000 - val_loss: 347079511.3210 - val_mse: 347079520.0000\\n\",\n",
        "      \"Epoch 7/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 853us/step - loss: 310952193.1636 - mse: 310952160.0000 - val_loss: 243565299.5384 - val_mse: 243565312.0000\\n\",\n",
        "      \"Epoch 8/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 870us/step - loss: 187825129.7986 - mse: 187825168.0000 - val_loss: 184602332.4057 - val_mse: 184602368.0000\\n\",\n",
        "      \"Epoch 9/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 900us/step - loss: 88138291.6951 - mse: 88138320.0000 - val_loss: 63708493.3298 - val_mse: 63708500.0000\\n\",\n",
        "      \"Epoch 10/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 912us/step - loss: 42614611.8319 - mse: 42614620.0000 - val_loss: 28011989.5984 - val_mse: 28011996.0000\\n\",\n",
        "      \"Epoch 11/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 925us/step - loss: 20934681.3956 - mse: 20934680.0000 - val_loss: 14581415.6072 - val_mse: 14581415.0000\\n\",\n",
        "      \"Epoch 12/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 917us/step - loss: 12801787.8911 - mse: 12801792.0000 - val_loss: 11809940.0959 - val_mse: 11809941.0000\\n\",\n",
        "      \"Epoch 13/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 917us/step - loss: 9491336.0359 - mse: 9491336.0000 - val_loss: 6245860.3957 - val_mse: 6245860.5000\\n\",\n",
        "      \"Epoch 14/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 874us/step - loss: 6910212.1797 - mse: 6910214.0000 - val_loss: 5355836.0043 - val_mse: 5355836.5000\\n\",\n",
        "      \"Epoch 15/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 903us/step - loss: 6440056.8108 - mse: 6440055.5000 - val_loss: 6119316.0618 - val_mse: 6119314.5000\\n\",\n",
        "      \"Epoch 16/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 921us/step - loss: 4612601.1873 - mse: 4612601.5000 - val_loss: 3702728.4067 - val_mse: 3702729.0000\\n\",\n",
        "      \"Epoch 17/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 912us/step - loss: 3449385.3221 - mse: 3449385.5000 - val_loss: 2804464.5906 - val_mse: 2804465.0000\\n\",\n",
        "      \"Epoch 18/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 912us/step - loss: 3228551.2351 - mse: 3228551.2500 - val_loss: 3026767.0920 - val_mse: 3026767.2500\\n\",\n",
        "      \"Epoch 19/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 904us/step - loss: 2660765.1027 - mse: 2660764.5000 - val_loss: 2174061.2944 - val_mse: 2174061.5000\\n\",\n",
        "      \"Epoch 20/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 921us/step - loss: 2332630.6509 - mse: 2332630.5000 - val_loss: 2742665.7276 - val_mse: 2742665.2500\\n\",\n",
        "      \"Epoch 21/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 926us/step - loss: 2038791.1775 - mse: 2038791.8750 - val_loss: 1530994.4149 - val_mse: 1530994.6250\\n\",\n",
        "      \"Epoch 22/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 913us/step - loss: 1621887.8828 - mse: 1621888.3750 - val_loss: 1346817.9259 - val_mse: 1346818.0000\\n\",\n",
        "      \"Epoch 23/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 912us/step - loss: 1922075.2860 - mse: 1922074.7500 - val_loss: 1487542.2031 - val_mse: 1487542.1250\\n\",\n",
        "      \"Epoch 24/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 908us/step - loss: 1530996.3784 - mse: 1530996.1250 - val_loss: 931092.1430 - val_mse: 931092.1875\\n\",\n",
        "      \"Epoch 25/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 900us/step - loss: 1388240.6773 - mse: 1388240.3750 - val_loss: 1631309.7620 - val_mse: 1631310.1250\\n\",\n",
        "      \"Epoch 26/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 921us/step - loss: 1193742.7680 - mse: 1193742.8750 - val_loss: 1277016.7235 - val_mse: 1277017.0000\\n\",\n",
        "      \"Epoch 27/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 908us/step - loss: 1499685.3111 - mse: 1499685.2500 - val_loss: 949654.4899 - val_mse: 949654.6250\\n\",\n",
        "      \"Epoch 28/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 908us/step - loss: 1047884.1864 - mse: 1047884.2500 - val_loss: 1222761.6000 - val_mse: 1222761.5000\\n\",\n",
        "      \"Epoch 29/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 912us/step - loss: 1309092.7143 - mse: 1309092.8750 - val_loss: 741060.9865 - val_mse: 741060.8125\\n\",\n",
        "      \"Epoch 30/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 912us/step - loss: 1225774.4567 - mse: 1225774.1250 - val_loss: 1389041.6025 - val_mse: 1389041.8750\\n\",\n",
        "      \"Epoch 31/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 917us/step - loss: 1373750.2624 - mse: 1373750.2500 - val_loss: 30239626.8063 - val_mse: 30239624.0000\\n\",\n",
        "      \"Epoch 32/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 925us/step - loss: 1896632.5557 - mse: 1896632.6250 - val_loss: 773488.7574 - val_mse: 773488.7500\\n\",\n",
        "      \"Epoch 33/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 908us/step - loss: 2869184.8767 - mse: 2869185.0000 - val_loss: 1275148.7795 - val_mse: 1275148.7500\\n\",\n",
        "      \"Epoch 34/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 900us/step - loss: 1539222.2870 - mse: 1539222.2500 - val_loss: 1122739.3411 - val_mse: 1122739.1250\\n\",\n",
        "      \"Epoch 35/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 908us/step - loss: 2359828.8985 - mse: 2359829.5000 - val_loss: 2033158.9993 - val_mse: 2033159.3750\\n\",\n",
        "      \"Epoch 36/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 917us/step - loss: 1178975.5135 - mse: 1178975.5000 - val_loss: 1448818.5111 - val_mse: 1448818.2500\\n\",\n",
        "      \"Epoch 37/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 909us/step - loss: 1428989.2974 - mse: 1428989.5000 - val_loss: 3833040.1755 - val_mse: 3833040.0000\\n\",\n",
        "      \"Epoch 38/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 925us/step - loss: 2841590.6461 - mse: 2841591.7500 - val_loss: 926346.0082 - val_mse: 926346.1875\\n\",\n",
        "      \"Epoch 39/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 917us/step - loss: 2483537.2575 - mse: 2483537.2500 - val_loss: 794689.2876 - val_mse: 794689.0625\\n\",\n",
        "      \"Epoch 40/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 917us/step - loss: 1111135.4366 - mse: 1111136.0000 - val_loss: 912915.2878 - val_mse: 912915.3125\\n\",\n",
        "      \"Epoch 41/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 908us/step - loss: 1682362.8409 - mse: 1682363.0000 - val_loss: 571293.8870 - val_mse: 571293.8125\\n\",\n",
        "      \"Epoch 42/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 887us/step - loss: 1007158.6787 - mse: 1007158.9375 - val_loss: 1001145.0979 - val_mse: 1001145.2500\\n\",\n",
        "      \"Epoch 43/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 913us/step - loss: 1224728.4894 - mse: 1224728.7500 - val_loss: 1557724.1836 - val_mse: 1557724.5000\\n\",\n",
        "      \"Epoch 44/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 921us/step - loss: 1566596.6554 - mse: 1566596.6250 - val_loss: 899767.9618 - val_mse: 899768.0000\\n\",\n",
        "      \"Epoch 45/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 909us/step - loss: 1157689.5004 - mse: 1157689.6250 - val_loss: 988396.5699 - val_mse: 988396.5625\\n\",\n",
        "      \"Epoch 46/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 917us/step - loss: 1063001.9092 - mse: 1063002.1250 - val_loss: 3983318.4908 - val_mse: 3983318.7500\\n\",\n",
        "      \"Epoch 47/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 917us/step - loss: 1314549.3835 - mse: 1314548.8750 - val_loss: 939247.8432 - val_mse: 939248.0000\\n\",\n",
        "      \"Epoch 48/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 908us/step - loss: 1928120.5056 - mse: 1928120.7500 - val_loss: 1227317.0598 - val_mse: 1227316.8750\\n\",\n",
        "      \"Epoch 49/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 921us/step - loss: 1546948.1989 - mse: 1546947.8750 - val_loss: 857590.3707 - val_mse: 857590.5000\\n\",\n",
        "      \"Epoch 50/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 897us/step - loss: 2813011.8825 - mse: 2813012.5000 - val_loss: 19431211.6958 - val_mse: 19431214.0000\\n\",\n",
        "      \"Epoch 51/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 819us/step - loss: 2669107.5521 - mse: 2669109.0000 - val_loss: 1003742.2247 - val_mse: 1003742.2500\\n\",\n",
        "      \"Epoch 52/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 857us/step - loss: 1450776.6200 - mse: 1450776.6250 - val_loss: 1649548.2304 - val_mse: 1649548.1250\\n\",\n",
        "      \"Epoch 53/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 870us/step - loss: 1194581.1821 - mse: 1194581.0000 - val_loss: 1400786.9028 - val_mse: 1400786.6250\\n\",\n",
        "      \"Epoch 54/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 895us/step - loss: 1078710.9990 - mse: 1078711.1250 - val_loss: 1693578.8097 - val_mse: 1693579.1250\\n\",\n",
        "      \"Epoch 55/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 900us/step - loss: 1178732.1061 - mse: 1178731.8750 - val_loss: 805105.3008 - val_mse: 805105.1250\\n\",\n",
        "      \"Epoch 56/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 883us/step - loss: 1115568.0945 - mse: 1115567.8750 - val_loss: 990801.9015 - val_mse: 990802.0625\\n\",\n",
        "      \"Epoch 57/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 896us/step - loss: 1081785.7113 - mse: 1081785.2500 - val_loss: 914682.9931 - val_mse: 914682.9375\\n\",\n",
        "      \"Epoch 58/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 891us/step - loss: 1222022.1859 - mse: 1222022.2500 - val_loss: 601125.0293 - val_mse: 601125.0000\\n\",\n",
        "      \"Epoch 59/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 894us/step - loss: 815629.0194 - mse: 815629.0000 - val_loss: 1167771.7199 - val_mse: 1167771.7500\\n\",\n",
        "      \"Epoch 60/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 887us/step - loss: 2162857.0002 - mse: 2162857.5000 - val_loss: 1407932.0099 - val_mse: 1407931.8750\\n\",\n",
        "      \"Epoch 61/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 900us/step - loss: 1429325.1187 - mse: 1429325.5000 - val_loss: 1461156.1755 - val_mse: 1461156.3750\\n\",\n",
        "      \"127/127 [==============================] - 0s 346us/step\\n\",\n",
        "      \"loss :  1041660.1117986244\\n\",\n",
        "      \"mse :  1041660.125\\n\"\n",
        "     ]\n",
        "    }\n",
        "   ],\n",
        "   \"source\": [\n",
        "    \"from keras.models import Sequential\\n\",\n",
        "    \"from keras.layers import Dense\\n\",\n",
        "    \"\\n\",\n",
        "    \"# 모델구성\\n\",\n",
        "    \"model = Sequential()\\n\",\n",
        "    \"model.add(Dense(64, input_shape=(25, )))\\n\",\n",
        "    \"model.add(Dense(32, activation='relu'))\\n\",\n",
        "    \"model.add(Dense(32, activation='relu'))\\n\",\n",
        "    \"model.add(Dense(32, activation='relu'))\\n\",\n",
        "    \"model.add(Dense(32, activation='relu'))\\n\",\n",
        "    \"model.add(Dense(1))\\n\",\n",
        "    \"\\n\",\n",
        "    \"model.compile(loss='mse', optimizer='adam', metrics=['mse'])\\n\",\n",
        "    \"\\n\",\n",
        "    \"from keras.callbacks import EarlyStopping\\n\",\n",
        "    \"\\n\",\n",
        "    \"early_stopping = EarlyStopping(patience=20)\\n\",\n",
        "    \"model.fit(x_train_scaled, y_train, validation_split=0.2, verbose=1,\\n\",\n",
        "    \"          batch_size=1, epochs=100, callbacks=[early_stopping])\\n\",\n",
        "    \"\\n\",\n",
        "    \"loss, mse = model.evaluate(x_test_scaled, y_test, batch_size=1)\\n\",\n",
        "    \"\\n\",\n",
        "    \"print('loss : ', loss)\\n\",\n",
        "    \"print('mse : ', mse)\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": 42,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [\n",
        "    {\n",
        "     \"name\": \"stdout\",\n",
        "     \"output_type\": \"stream\",\n",
        "     \"text\": [\n",
        "      \"종가 :  [52200] / 예측가 :  [51060.69]\\n\",\n",
        "      \"종가 :  [41450] / 예측가 :  [41180.883]\\n\",\n",
        "      \"종가 :  [49650] / 예측가 :  [49299.266]\\n\",\n",
        "      \"종가 :  [44800] / 예측가 :  [44759.64]\\n\",\n",
        "      \"종가 :  [49500] / 예측가 :  [48002.824]\\n\"\n",
        "     ]\n",
        "    }\n",
        "   ],\n",
        "   \"source\": [\n",
        "    \"y_pred = model.predict(x_test_scaled)\\n\",\n",
        "    \"\\n\",\n",
        "    \"for i in range(5):\\n\",\n",
        "    \"    print('종가 : ', y_test[i], '/ 예측가 : ', y_pred[i])\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": 43,\n",
        "   \"metadata\": {\n",
        "    \"scrolled\": true\n",
        "   },\n",
        "   \"outputs\": [\n",
        "    {\n",
        "     \"name\": \"stdout\",\n",
        "     \"output_type\": \"stream\",\n",
        "     \"text\": [\n",
        "      \"[ 1.13392563  1.07633086  1.02393741  0.93187013 -0.028819    1.05994459\\n\",\n",
        "      \"  1.04756614  1.06686328  0.94933344  1.53138774  0.79093879  0.80213886\\n\",\n",
        "      \"  0.84540479  0.81507608  0.10811605  0.73712199  0.68893234  0.71073997\\n\",\n",
        "      \"  0.69657856  0.37360361  0.85647275  0.80021877  0.80910994  0.69442952\\n\",\n",
        "      \" -0.16973555]\\n\"\n",
        "     ]\n",
        "    }\n",
        "   ],\n",
        "   \"source\": [\n",
        "    \"# sam03_lstm.py\\n\",\n",
        "    \"    \\n\",\n",
        "    \"\\n\",\n",
        "    \"def split_xy5(dataset, time_steps, y_column):\\n\",\n",
        "    \"    x, y = list(), list()\\n\",\n",
        "    \"    for i in range(len(dataset)):\\n\",\n",
        "    \"        x_end_number = i + time_steps\\n\",\n",
        "    \"        y_end_number = x_end_number + y_column # 수정\\n\",\n",
        "    \"\\n\",\n",
        "    \"        if y_end_number > len(dataset):  # 수정\\n\",\n",
        "    \"            break\\n\",\n",
        "    \"        tmp_x = dataset[i:x_end_number, :]  # 수정\\n\",\n",
        "    \"        tmp_y = dataset[x_end_number:y_end_number, 3]    # 수정\\n\",\n",
        "    \"        x.append(tmp_x)\\n\",\n",
        "    \"        y.append(tmp_y)\\n\",\n",
        "    \"\\n\",\n",
        "    \"    return np.array(x), np.array(y)\\n\",\n",
        "    \"\\n\",\n",
        "    \"x, y = split_xy5(samsung, 5, 1) \\n\",\n",
        "    \"print(x[0,:], \\\"\\\\n\\\", y[0])\\n\",\n",
        "    \"print(x.shape)\\n\",\n",
        "    \"print(y.shape)\\n\",\n",
        "    \"\\n\",\n",
        "    \"# 데이터 셋 나누기\\n\",\n",
        "    \"from sklearn.model_selection import train_test_split\\n\",\n",
        "    \"# from sklearn.model_selection import cross_val_score\\n\",\n",
        "    \"x_train, x_test, y_train, y_test = train_test_split(\\n\",\n",
        "    \"    x, y, random_state=1, test_size = 0.3)\\n\",\n",
        "    \"\\n\",\n",
        "    \"print(x_train.shape)\\n\",\n",
        "    \"print(x_test.shape)\\n\",\n",
        "    \"print(y_train.shape)\\n\",\n",
        "    \"print(y_test.shape)\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": null,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"x_train = np.reshape(x_train,\\n\",\n",
        "    \"    (x_train.shape[0], x_train.shape[1] * x_train.shape[2]))\\n\",\n",
        "    \"x_test = np.reshape(x_test,\\n\",\n",
        "    \"    (x_test.shape[0], x_test.shape[1] * x_test.shape[2]))\\n\",\n",
        "    \"\\n\",\n",
        "    \"print(x_train.shape)\\n\",\n",
        "    \"print(x_test.shape)\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": null,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"#### 데이터 전처리 #####\\n\",\n",
        "    \"from sklearn.preprocessing import StandardScaler\\n\",\n",
        "    \"\\n\",\n",
        "    \"scaler = StandardScaler()\\n\",\n",
        "    \"scaler.fit(x_train)\\n\",\n",
        "    \"\\n\",\n",
        "    \"x_train_scaled = scaler.transform(x_train)\\n\",\n",
        "    \"x_test_scaled = scaler.transform(x_test)\\n\",\n",
        "    \"\\n\",\n",
        "    \"print(x_train_scaled[0, :])\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": 44,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [\n",
        "    {\n",
        "     \"name\": \"stdout\",\n",
        "     \"output_type\": \"stream\",\n",
        "     \"text\": [\n",
        "      \"(294, 5, 5)\\n\",\n",
        "      \"(127, 5, 5)\\n\"\n",
        "     ]\n",
        "    }\n",
        "   ],\n",
        "   \"source\": [\n",
        "    \"x_train_scaled = np.reshape(x_train_scaled,\\n\",\n",
        "    \"    (x_train_scaled.shape[0], 5, 5))\\n\",\n",
        "    \"x_test_scaled = np.reshape(x_test_scaled,\\n\",\n",
        "    \"    (x_test_scaled.shape[0], 5, 5))\\n\",\n",
        "    \"\\n\",\n",
        "    \"print(x_train_scaled.shape)\\n\",\n",
        "    \"print(x_test_scaled.shape)\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": 45,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [\n",
        "    {\n",
        "     \"name\": \"stdout\",\n",
        "     \"output_type\": \"stream\",\n",
        "     \"text\": [\n",
        "      \"Train on 235 samples, validate on 59 samples\\n\",\n",
        "      \"Epoch 1/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 3ms/step - loss: 1848203808.5191 - mse: 1848204160.0000 - val_loss: 687527351.2407 - val_mse: 687527296.0000\\n\",\n",
        "      \"Epoch 2/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 261144979.6052 - mse: 261144960.0000 - val_loss: 10786767.8815 - val_mse: 10786768.0000\\n\",\n",
        "      \"Epoch 3/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 15201580.1635 - mse: 15201578.0000 - val_loss: 5700398.6066 - val_mse: 5700397.5000\\n\",\n",
        "      \"Epoch 4/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 8606091.5743 - mse: 8606091.0000 - val_loss: 6508834.1182 - val_mse: 6508834.5000\\n\",\n",
        "      \"Epoch 5/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 13906158.2760 - mse: 13906159.0000 - val_loss: 6191275.6355 - val_mse: 6191276.5000\\n\",\n",
        "      \"Epoch 6/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 7998069.9731 - mse: 7998069.0000 - val_loss: 5057363.6973 - val_mse: 5057364.0000\\n\",\n",
        "      \"Epoch 7/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 4882129.6330 - mse: 4882131.0000 - val_loss: 8269247.7606 - val_mse: 8269247.5000\\n\",\n",
        "      \"Epoch 8/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 4552857.0508 - mse: 4552856.5000 - val_loss: 6027695.5185 - val_mse: 6027697.0000\\n\",\n",
        "      \"Epoch 9/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 7698569.5367 - mse: 7698572.0000 - val_loss: 3650651.4716 - val_mse: 3650650.7500\\n\",\n",
        "      \"Epoch 10/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 2702417.9748 - mse: 2702417.7500 - val_loss: 4596501.1686 - val_mse: 4596500.5000\\n\",\n",
        "      \"Epoch 11/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 1868624.1598 - mse: 1868624.2500 - val_loss: 3898632.8431 - val_mse: 3898632.5000\\n\",\n",
        "      \"Epoch 12/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 1627511.0869 - mse: 1627510.8750 - val_loss: 3653150.6748 - val_mse: 3653151.2500\\n\",\n",
        "      \"Epoch 13/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 1464347.2403 - mse: 1464347.2500 - val_loss: 3913650.2803 - val_mse: 3913650.5000\\n\",\n",
        "      \"Epoch 14/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 1432254.1757 - mse: 1432254.1250 - val_loss: 3363813.6188 - val_mse: 3363814.2500\\n\",\n",
        "      \"Epoch 15/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 1516193.4188 - mse: 1516192.6250 - val_loss: 3128188.9025 - val_mse: 3128189.5000\\n\",\n",
        "      \"Epoch 16/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 1285810.5919 - mse: 1285810.6250 - val_loss: 3349043.6456 - val_mse: 3349043.0000\\n\",\n",
        "      \"Epoch 17/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 1327587.2017 - mse: 1327587.0000 - val_loss: 2918061.0377 - val_mse: 2918060.7500\\n\",\n",
        "      \"Epoch 18/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 1134027.3970 - mse: 1134027.5000 - val_loss: 2693755.1495 - val_mse: 2693755.0000\\n\",\n",
        "      \"Epoch 19/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 1108825.0634 - mse: 1108825.3750 - val_loss: 2477057.3285 - val_mse: 2477056.7500\\n\",\n",
        "      \"Epoch 20/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 1042326.6065 - mse: 1042326.3125 - val_loss: 2974746.3146 - val_mse: 2974746.2500\\n\",\n",
        "      \"Epoch 21/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 1186386.8252 - mse: 1186387.1250 - val_loss: 2574659.2597 - val_mse: 2574659.2500\\n\",\n",
        "      \"Epoch 22/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 1093267.2539 - mse: 1093266.8750 - val_loss: 2674322.0468 - val_mse: 2674321.2500\\n\",\n",
        "      \"Epoch 23/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 1041702.7050 - mse: 1041702.8125 - val_loss: 2422720.3180 - val_mse: 2422720.5000\\n\",\n",
        "      \"Epoch 24/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 1137727.7620 - mse: 1137727.8750 - val_loss: 2461181.9276 - val_mse: 2461182.7500\\n\",\n",
        "      \"Epoch 25/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 924043.8036 - mse: 924043.6250 - val_loss: 2370914.8417 - val_mse: 2370915.5000\\n\",\n",
        "      \"Epoch 26/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 948776.6199 - mse: 948776.5625 - val_loss: 2458784.9186 - val_mse: 2458785.0000\\n\",\n",
        "      \"Epoch 27/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 1089662.0502 - mse: 1089662.3750 - val_loss: 2259148.5917 - val_mse: 2259148.7500\\n\",\n",
        "      \"Epoch 28/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 896712.0551 - mse: 896711.8750 - val_loss: 2419111.0954 - val_mse: 2419111.0000\\n\",\n",
        "      \"Epoch 29/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 940770.7817 - mse: 940770.6250 - val_loss: 1718101.3248 - val_mse: 1718101.1250\\n\",\n",
        "      \"Epoch 30/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 1078701.8330 - mse: 1078701.5000 - val_loss: 1893098.9380 - val_mse: 1893098.5000\\n\",\n",
        "      \"Epoch 31/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 1103742.8042 - mse: 1103742.8750 - val_loss: 3709010.1964 - val_mse: 3709010.2500\\n\",\n",
        "      \"Epoch 32/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 1015368.8941 - mse: 1015368.5625 - val_loss: 1765324.6965 - val_mse: 1765325.0000\\n\",\n",
        "      \"Epoch 33/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 1155425.0492 - mse: 1155425.2500 - val_loss: 1970673.6184 - val_mse: 1970673.7500\\n\",\n",
        "      \"Epoch 34/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 962861.0685 - mse: 962861.1250 - val_loss: 1662775.2799 - val_mse: 1662775.3750\\n\",\n",
        "      \"Epoch 35/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 876038.5850 - mse: 876038.8125 - val_loss: 2155475.3745 - val_mse: 2155475.2500\\n\",\n",
        "      \"Epoch 36/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 876524.3438 - mse: 876524.1250 - val_loss: 3593465.7551 - val_mse: 3593466.2500\\n\",\n",
        "      \"Epoch 37/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 1023968.1562 - mse: 1023968.5625 - val_loss: 1495332.5168 - val_mse: 1495332.1250\\n\",\n",
        "      \"Epoch 38/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 912457.0993 - mse: 912457.0000 - val_loss: 1604153.8677 - val_mse: 1604153.8750\\n\",\n",
        "      \"Epoch 39/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 830356.5150 - mse: 830356.3125 - val_loss: 1556783.8109 - val_mse: 1556783.6250\\n\",\n",
        "      \"Epoch 40/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 855249.2915 - mse: 855249.3125 - val_loss: 1768983.6058 - val_mse: 1768983.6250\\n\",\n",
        "      \"Epoch 41/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 939606.8863 - mse: 939607.0625 - val_loss: 2460768.4524 - val_mse: 2460768.2500\\n\",\n",
        "      \"Epoch 42/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 1006633.4374 - mse: 1006633.3750 - val_loss: 1030848.4161 - val_mse: 1030848.4375\\n\",\n",
        "      \"Epoch 43/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 951697.6622 - mse: 951697.6875 - val_loss: 2443432.1135 - val_mse: 2443431.7500\\n\",\n",
        "      \"Epoch 44/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 1655364.7151 - mse: 1655364.1250 - val_loss: 735201.4015 - val_mse: 735201.5000\\n\",\n",
        "      \"Epoch 45/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 1486507.4633 - mse: 1486507.1250 - val_loss: 1064257.6472 - val_mse: 1064257.8750\\n\",\n",
        "      \"Epoch 46/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 5543689.2314 - mse: 5543690.5000 - val_loss: 2312357.2130 - val_mse: 2312357.0000\\n\",\n",
        "      \"Epoch 47/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 1209682.3637 - mse: 1209682.6250 - val_loss: 706138.3019 - val_mse: 706138.5000\\n\",\n",
        "      \"Epoch 48/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 768799.5000 - mse: 768799.3750 - val_loss: 999460.5754 - val_mse: 999460.7500\\n\",\n",
        "      \"Epoch 49/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 856512.6569 - mse: 856512.8750 - val_loss: 3229921.4783 - val_mse: 3229921.7500\\n\",\n",
        "      \"Epoch 50/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 1043756.6401 - mse: 1043756.7500 - val_loss: 1090489.9553 - val_mse: 1090490.1250\\n\",\n",
        "      \"Epoch 51/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 947584.0470 - mse: 947583.9375 - val_loss: 1442358.0182 - val_mse: 1442357.8750\\n\",\n",
        "      \"Epoch 52/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 862993.1741 - mse: 862993.5000 - val_loss: 1540729.5966 - val_mse: 1540729.6250\\n\",\n",
        "      \"Epoch 53/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 731800.7357 - mse: 731800.8750 - val_loss: 836505.5585 - val_mse: 836505.5000\\n\",\n",
        "      \"Epoch 54/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 834600.8358 - mse: 834600.5625 - val_loss: 1061339.0010 - val_mse: 1061339.0000\\n\",\n",
        "      \"Epoch 55/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 872589.4684 - mse: 872589.5000 - val_loss: 757662.6697 - val_mse: 757662.8750\\n\",\n",
        "      \"Epoch 56/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 871170.6155 - mse: 871170.5000 - val_loss: 748172.0339 - val_mse: 748172.0625\\n\",\n",
        "      \"Epoch 57/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 918535.5242 - mse: 918535.3750 - val_loss: 1476251.6395 - val_mse: 1476251.5000\\n\",\n",
        "      \"Epoch 58/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 1166278.3161 - mse: 1166278.3750 - val_loss: 1431089.2373 - val_mse: 1431089.3750\\n\",\n",
        "      \"Epoch 59/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 945600.1130 - mse: 945600.1875 - val_loss: 895616.7086 - val_mse: 895616.7500\\n\",\n",
        "      \"Epoch 60/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 845066.8245 - mse: 845066.8750 - val_loss: 877719.0631 - val_mse: 877719.0625\\n\",\n",
        "      \"Epoch 61/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 833901.4091 - mse: 833901.1875 - val_loss: 717265.3408 - val_mse: 717265.3750\\n\",\n",
        "      \"Epoch 62/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 737087.5781 - mse: 737087.5000 - val_loss: 1348295.5817 - val_mse: 1348295.7500\\n\",\n",
        "      \"Epoch 63/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 822022.4043 - mse: 822022.5625 - val_loss: 1072878.3918 - val_mse: 1072878.3750\\n\",\n",
        "      \"Epoch 64/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 827720.4973 - mse: 827720.3125 - val_loss: 841431.2992 - val_mse: 841431.4375\\n\",\n",
        "      \"Epoch 65/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 855924.4855 - mse: 855924.6250 - val_loss: 660772.5265 - val_mse: 660772.5625\\n\",\n",
        "      \"Epoch 66/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 1030987.4641 - mse: 1030987.4375 - val_loss: 784647.7364 - val_mse: 784647.7500\\n\",\n",
        "      \"Epoch 67/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 784579.2131 - mse: 784579.3125 - val_loss: 887028.1797 - val_mse: 887028.3125\\n\",\n",
        "      \"Epoch 68/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 795674.2312 - mse: 795673.8750 - val_loss: 1192752.2482 - val_mse: 1192752.3750\\n\",\n",
        "      \"Epoch 69/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 822185.5948 - mse: 822185.4375 - val_loss: 1555138.4772 - val_mse: 1555138.0000\\n\",\n",
        "      \"Epoch 70/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 831435.7444 - mse: 831435.6875 - val_loss: 996733.8508 - val_mse: 996733.6875\\n\",\n",
        "      \"Epoch 71/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 920408.1320 - mse: 920408.3750 - val_loss: 1027728.8196 - val_mse: 1027728.8125\\n\",\n",
        "      \"Epoch 72/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 862518.9327 - mse: 862518.9375 - val_loss: 603924.8525 - val_mse: 603924.8125\\n\",\n",
        "      \"Epoch 73/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 786310.4290 - mse: 786310.7500 - val_loss: 976320.3578 - val_mse: 976320.3125\\n\",\n",
        "      \"Epoch 74/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 1326481.7257 - mse: 1326481.7500 - val_loss: 772094.9722 - val_mse: 772094.9375\\n\",\n",
        "      \"Epoch 75/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 975383.9600 - mse: 975383.6875 - val_loss: 966992.9160 - val_mse: 966993.0000\\n\",\n",
        "      \"Epoch 76/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 809016.7180 - mse: 809016.8125 - val_loss: 1710458.3831 - val_mse: 1710458.2500\\n\",\n",
        "      \"Epoch 77/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 696061.4256 - mse: 696061.1875 - val_loss: 1315656.2799 - val_mse: 1315656.1250\\n\",\n",
        "      \"Epoch 78/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 857400.7589 - mse: 857400.6250 - val_loss: 1065309.8868 - val_mse: 1065310.1250\\n\",\n",
        "      \"Epoch 79/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 823466.3567 - mse: 823466.0625 - val_loss: 761956.4627 - val_mse: 761956.6250\\n\",\n",
        "      \"Epoch 80/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 914270.9231 - mse: 914271.0000 - val_loss: 1160518.6591 - val_mse: 1160518.6250\\n\",\n",
        "      \"Epoch 81/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 781828.7708 - mse: 781828.8125 - val_loss: 840372.6342 - val_mse: 840372.7500\\n\",\n",
        "      \"Epoch 82/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 825989.6805 - mse: 825989.5000 - val_loss: 787050.0596 - val_mse: 787049.9375\\n\",\n",
        "      \"Epoch 83/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 908242.7214 - mse: 908242.8125 - val_loss: 823718.6994 - val_mse: 823718.8750\\n\",\n",
        "      \"Epoch 84/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 767285.1845 - mse: 767285.1250 - val_loss: 936265.0810 - val_mse: 936265.1250\\n\",\n",
        "      \"Epoch 85/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 920640.7652 - mse: 920640.6250 - val_loss: 790678.8488 - val_mse: 790678.8750\\n\",\n",
        "      \"Epoch 86/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 2619796.2517 - mse: 2619796.2500 - val_loss: 1073384.9134 - val_mse: 1073384.8750\\n\",\n",
        "      \"Epoch 87/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 853107.7783 - mse: 853107.9375 - val_loss: 988529.5343 - val_mse: 988529.5625\\n\",\n",
        "      \"Epoch 88/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 782726.8711 - mse: 782726.9375 - val_loss: 722038.2743 - val_mse: 722038.1250\\n\",\n",
        "      \"Epoch 89/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 823289.7699 - mse: 823289.8750 - val_loss: 1662766.8889 - val_mse: 1662766.7500\\n\",\n",
        "      \"Epoch 90/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 873078.8622 - mse: 873078.8125 - val_loss: 1077861.2566 - val_mse: 1077861.0000\\n\",\n",
        "      \"Epoch 91/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 1088739.8958 - mse: 1088739.6250 - val_loss: 605513.8402 - val_mse: 605513.8125\\n\",\n",
        "      \"Epoch 92/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 717749.3960 - mse: 717749.3750 - val_loss: 630707.2445 - val_mse: 630707.0625\\n\",\n",
        "      \"127/127 [==============================] - 0s 629us/step\\n\",\n",
        "      \"loss :  707549.3071851354\\n\",\n",
        "      \"mse :  707549.375\\n\"\n",
        "     ]\n",
        "    }\n",
        "   ],\n",
        "   \"source\": [\n",
        "    \"from keras.models import Sequential\\n\",\n",
        "    \"from keras.layers import Dense, LSTM\\n\",\n",
        "    \"\\n\",\n",
        "    \"# 모델구성\\n\",\n",
        "    \"model = Sequential()\\n\",\n",
        "    \"model.add(LSTM(64, input_shape=(5, 5)))\\n\",\n",
        "    \"model.add(Dense(32, activation='relu'))\\n\",\n",
        "    \"model.add(Dense(32, activation='relu'))\\n\",\n",
        "    \"model.add(Dense(32, activation='relu'))\\n\",\n",
        "    \"model.add(Dense(32, activation='relu'))\\n\",\n",
        "    \"model.add(Dense(1))\\n\",\n",
        "    \"\\n\",\n",
        "    \"model.compile(loss='mse', optimizer='adam', metrics=['mse'])\\n\",\n",
        "    \"\\n\",\n",
        "    \"from keras.callbacks import EarlyStopping\\n\",\n",
        "    \"\\n\",\n",
        "    \"early_stopping = EarlyStopping(patience=20)\\n\",\n",
        "    \"model.fit(x_train_scaled, y_train, validation_split=0.2, verbose=1,\\n\",\n",
        "    \"          batch_size=1, epochs=100, callbacks=[early_stopping])\\n\",\n",
        "    \"\\n\",\n",
        "    \"loss, mse = model.evaluate(x_test_scaled, y_test, batch_size=1)\\n\",\n",
        "    \"\\n\",\n",
        "    \"print('loss : ', loss)\\n\",\n",
        "    \"print('mse : ', mse)\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": 46,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [\n",
        "    {\n",
        "     \"name\": \"stdout\",\n",
        "     \"output_type\": \"stream\",\n",
        "     \"text\": [\n",
        "      \"종가 :  [52200] / 예측가 :  [53023.32]\\n\",\n",
        "      \"종가 :  [41450] / 예측가 :  [40845.62]\\n\",\n",
        "      \"종가 :  [49650] / 예측가 :  [51283.984]\\n\",\n",
        "      \"종가 :  [44800] / 예측가 :  [46522.29]\\n\",\n",
        "      \"종가 :  [49500] / 예측가 :  [49850.14]\\n\"\n",
        "     ]\n",
        "    }\n",
        "   ],\n",
        "   \"source\": [\n",
        "    \"y_pred = model.predict(x_test_scaled)\\n\",\n",
        "    \"\\n\",\n",
        "    \"for i in range(5):\\n\",\n",
        "    \"    print('종가 : ', y_test[i], '/ 예측가 : ', y_pred[i])\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": 47,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [\n",
        "    {\n",
        "     \"name\": \"stdout\",\n",
        "     \"output_type\": \"stream\",\n",
        "     \"text\": [\n",
        "      \"[[   320.56    321.      316.75    316.75 173911.  ]\\n\",\n",
        "      \" [   317.79    319.53    314.95    315.87 141252.  ]\\n\",\n",
        "      \" [   315.93    316.08    312.6     314.42 123610.  ]\\n\",\n",
        "      \" [   316.51    317.34    315.11    317.31 120900.  ]\\n\",\n",
        "      \" [   318.03    319.7     317.86    318.51 112400.  ]] \\n\",\n",
        "      \" [317.72]\\n\",\n",
        "      \"(421, 5, 5)\\n\",\n",
        "      \"(421, 1)\\n\"\n",
        "     ]\n",
        "    }\n",
        "   ],\n",
        "   \"source\": [\n",
        "    \"# sam04_ensemble.py\\n\",\n",
        "    \"    \\n\",\n",
        "    \"def split_xy5(dataset, time_steps, y_column):\\n\",\n",
        "    \"    x, y = list(), list()\\n\",\n",
        "    \"    for i in range(len(dataset)):\\n\",\n",
        "    \"        x_end_number = i + time_steps\\n\",\n",
        "    \"        y_end_number = x_end_number + y_column # 수정\\n\",\n",
        "    \"\\n\",\n",
        "    \"        if y_end_number > len(dataset):  # 수정\\n\",\n",
        "    \"            break\\n\",\n",
        "    \"        tmp_x = dataset[i:x_end_number, :]  # 수정\\n\",\n",
        "    \"        tmp_y = dataset[x_end_number:y_end_number, 3]    # 수정\\n\",\n",
        "    \"        x.append(tmp_x)\\n\",\n",
        "    \"        y.append(tmp_y)\\n\",\n",
        "    \"        \\n\",\n",
        "    \"    return np.array(x), np.array(y)\\n\",\n",
        "    \"\\n\",\n",
        "    \"x1, y1 = split_xy5(samsung, 5, 1) \\n\",\n",
        "    \"x2, y2 = split_xy5(kospi200, 5, 1) \\n\",\n",
        "    \"\\n\",\n",
        "    \"print(x2[0,:], \\\"\\\\n\\\", y2[0])\\n\",\n",
        "    \"print(x2.shape)\\n\",\n",
        "    \"print(y2.shape)\\n\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": 48,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [\n",
        "    {\n",
        "     \"name\": \"stdout\",\n",
        "     \"output_type\": \"stream\",\n",
        "     \"text\": [\n",
        "      \"(294, 5, 5)\\n\",\n",
        "      \"(127, 5, 5)\\n\",\n",
        "      \"(294, 1)\\n\",\n",
        "      \"(127, 1)\\n\"\n",
        "     ]\n",
        "    }\n",
        "   ],\n",
        "   \"source\": [\n",
        "    \"# 데이터 셋 나누기\\n\",\n",
        "    \"from sklearn.model_selection import train_test_split\\n\",\n",
        "    \"# from sklearn.model_selection import cross_val_score\\n\",\n",
        "    \"x1_train, x1_test, y1_train, y1_test = train_test_split(\\n\",\n",
        "    \"    x1, y1, random_state=1, test_size = 0.3)\\n\",\n",
        "    \"x2_train, x2_test, y2_train, y2_test = train_test_split(\\n\",\n",
        "    \"    x2, y2, random_state=2, test_size = 0.3)\\n\",\n",
        "    \"\\n\",\n",
        "    \"print(x2_train.shape)\\n\",\n",
        "    \"print(x2_test.shape)\\n\",\n",
        "    \"print(y2_train.shape)\\n\",\n",
        "    \"print(y2_test.shape)\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": 49,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [\n",
        "    {\n",
        "     \"name\": \"stdout\",\n",
        "     \"output_type\": \"stream\",\n",
        "     \"text\": [\n",
        "      \"(294, 25)\\n\",\n",
        "      \"(127, 25)\\n\"\n",
        "     ]\n",
        "    }\n",
        "   ],\n",
        "   \"source\": [\n",
        "    \"x1_train = np.reshape(x1_train,\\n\",\n",
        "    \"    (x1_train.shape[0], x1_train.shape[1] * x1_train.shape[2]))\\n\",\n",
        "    \"x1_test = np.reshape(x1_test,\\n\",\n",
        "    \"    (x1_test.shape[0], x1_test.shape[1] * x1_test.shape[2]))\\n\",\n",
        "    \"x2_train = np.reshape(x2_train,\\n\",\n",
        "    \"    (x2_train.shape[0], x2_train.shape[1] * x2_train.shape[2]))\\n\",\n",
        "    \"x2_test = np.reshape(x2_test,\\n\",\n",
        "    \"    (x2_test.shape[0], x2_test.shape[1] * x2_test.shape[2]))\\n\",\n",
        "    \"\\n\",\n",
        "    \"print(x2_train.shape)\\n\",\n",
        "    \"print(x2_test.shape)\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": 50,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [\n",
        "    {\n",
        "     \"name\": \"stdout\",\n",
        "     \"output_type\": \"stream\",\n",
        "     \"text\": [\n",
        "      \"[ 0.58843327  0.533494    0.64328272  0.57661608 -0.49913908  0.60996636\\n\",\n",
        "      \"  0.54270686  0.36243605  0.29103458  0.25643568  0.37564145  0.30537859\\n\",\n",
        "      \"  0.39643403  0.3025121  -0.92269429  0.30427131  0.31328749  0.31314525\\n\",\n",
        "      \"  0.30057603 -1.24269186  0.29584292  0.2699222   0.37789775  0.34761784\\n\",\n",
        "      \" -0.73601691]\\n\"\n",
        "     ]\n",
        "    }\n",
        "   ],\n",
        "   \"source\": [\n",
        "    \"#### 데이터 전처리 #####\\n\",\n",
        "    \"\\n\",\n",
        "    \"from sklearn.preprocessing import StandardScaler\\n\",\n",
        "    \"\\n\",\n",
        "    \"scaler1 = StandardScaler()\\n\",\n",
        "    \"scaler1.fit(x1_train)\\n\",\n",
        "    \"\\n\",\n",
        "    \"x1_train_scaled = scaler1.transform(x1_train)\\n\",\n",
        "    \"x1_test_scaled = scaler1.transform(x1_test)\\n\",\n",
        "    \"\\n\",\n",
        "    \"scaler2 = StandardScaler()\\n\",\n",
        "    \"scaler2.fit(x2_train)\\n\",\n",
        "    \"\\n\",\n",
        "    \"x2_train_scaled = scaler2.transform(x2_train)\\n\",\n",
        "    \"x2_test_scaled = scaler2.transform(x2_test)\\n\",\n",
        "    \"\\n\",\n",
        "    \"print(x2_train_scaled[0, :])\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": 51,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [\n",
        "    {\n",
        "     \"name\": \"stdout\",\n",
        "     \"output_type\": \"stream\",\n",
        "     \"text\": [\n",
        "      \"Train on 235 samples, validate on 59 samples\\n\",\n",
        "      \"Epoch 1/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 1533124146.8003 - mse: 1533123328.0000 - val_loss: 28595802.1769 - val_mse: 28595806.0000\\n\",\n",
        "      \"Epoch 2/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 9813647.4317 - mse: 9813642.0000 - val_loss: 2907907.7043 - val_mse: 2907907.5000\\n\",\n",
        "      \"Epoch 3/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 3534822.2918 - mse: 3534821.7500 - val_loss: 1737839.4918 - val_mse: 1737839.1250\\n\",\n",
        "      \"Epoch 4/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 2996008.7863 - mse: 2996008.7500 - val_loss: 1355658.5795 - val_mse: 1355658.7500\\n\",\n",
        "      \"Epoch 5/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 7602945.4808 - mse: 7602945.0000 - val_loss: 4074364.2529 - val_mse: 4074364.5000\\n\",\n",
        "      \"Epoch 6/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 2734073.0904 - mse: 2734072.7500 - val_loss: 1228953.2611 - val_mse: 1228953.1250\\n\",\n",
        "      \"Epoch 7/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 5274176.8297 - mse: 5274177.5000 - val_loss: 1790824.3963 - val_mse: 1790824.2500\\n\",\n",
        "      \"Epoch 8/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 2898401.7430 - mse: 2898401.0000 - val_loss: 9453991.1565 - val_mse: 9453990.0000\\n\",\n",
        "      \"Epoch 9/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 3313372.2166 - mse: 3313372.7500 - val_loss: 1672803.0717 - val_mse: 1672803.0000\\n\",\n",
        "      \"Epoch 10/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 8124367.4426 - mse: 8124372.0000 - val_loss: 1762826.8179 - val_mse: 1762826.7500\\n\",\n",
        "      \"Epoch 11/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 4186360.0496 - mse: 4186360.0000 - val_loss: 5385500.8768 - val_mse: 5385500.0000\\n\",\n",
        "      \"Epoch 12/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 2140128.8211 - mse: 2140128.5000 - val_loss: 2492954.6558 - val_mse: 2492955.0000\\n\",\n",
        "      \"Epoch 13/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 5863030.7502 - mse: 5863030.0000 - val_loss: 5397329.1360 - val_mse: 5397329.5000\\n\",\n",
        "      \"Epoch 14/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 3133066.1280 - mse: 3133067.2500 - val_loss: 1551194.4483 - val_mse: 1551194.6250\\n\",\n",
        "      \"Epoch 15/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 3899665.3530 - mse: 3899665.7500 - val_loss: 5420849.0000 - val_mse: 5420849.5000\\n\",\n",
        "      \"Epoch 16/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 2147492.2006 - mse: 2147492.2500 - val_loss: 1054381.7859 - val_mse: 1054381.7500\\n\",\n",
        "      \"Epoch 17/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 3400168.7033 - mse: 3400168.2500 - val_loss: 1506919.8595 - val_mse: 1506920.0000\\n\",\n",
        "      \"Epoch 18/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 3615042.1683 - mse: 3615041.7500 - val_loss: 2035296.3740 - val_mse: 2035296.0000\\n\",\n",
        "      \"Epoch 19/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 4321611.4937 - mse: 4321609.0000 - val_loss: 1528045.9589 - val_mse: 1528045.7500\\n\",\n",
        "      \"Epoch 20/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 22489044.3539 - mse: 22489050.0000 - val_loss: 7990601.3072 - val_mse: 7990601.0000\\n\",\n",
        "      \"Epoch 21/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 2857468.7896 - mse: 2857467.7500 - val_loss: 871852.5713 - val_mse: 871852.6875\\n\",\n",
        "      \"Epoch 22/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 1231890.0394 - mse: 1231889.7500 - val_loss: 1730142.4897 - val_mse: 1730142.5000\\n\",\n",
        "      \"Epoch 23/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 1522208.2191 - mse: 1522208.1250 - val_loss: 1436771.6683 - val_mse: 1436771.6250\\n\",\n",
        "      \"Epoch 24/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 5859224.0364 - mse: 5859221.0000 - val_loss: 1445577.3545 - val_mse: 1445577.7500\\n\",\n",
        "      \"Epoch 25/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 4771857.4023 - mse: 4771858.0000 - val_loss: 1512222.0941 - val_mse: 1512222.1250\\n\",\n",
        "      \"Epoch 26/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 1686997.9873 - mse: 1686997.8750 - val_loss: 906917.8914 - val_mse: 906917.8750\\n\",\n",
        "      \"Epoch 27/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 2092956.6538 - mse: 2092956.7500 - val_loss: 4229291.7378 - val_mse: 4229291.5000\\n\",\n",
        "      \"Epoch 28/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 2808769.8855 - mse: 2808768.7500 - val_loss: 1476309.1019 - val_mse: 1476309.5000\\n\",\n",
        "      \"Epoch 29/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 1705226.1917 - mse: 1705226.5000 - val_loss: 1488175.9433 - val_mse: 1488175.8750\\n\",\n",
        "      \"Epoch 30/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 5778950.3651 - mse: 5778950.5000 - val_loss: 949093.9496 - val_mse: 949094.0625\\n\",\n",
        "      \"Epoch 31/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 2274106.7797 - mse: 2274106.7500 - val_loss: 2327901.8160 - val_mse: 2327902.2500\\n\",\n",
        "      \"Epoch 32/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 1580551.3583 - mse: 1580552.0000 - val_loss: 1475990.3467 - val_mse: 1475990.0000\\n\",\n",
        "      \"Epoch 33/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 4824461.2961 - mse: 4824461.5000 - val_loss: 18010148.9231 - val_mse: 18010152.0000\\n\",\n",
        "      \"Epoch 34/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 6977722.2214 - mse: 6977722.5000 - val_loss: 1093678.2523 - val_mse: 1093678.0000\\n\",\n",
        "      \"Epoch 35/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 1457619.4042 - mse: 1457619.7500 - val_loss: 1147355.7939 - val_mse: 1147355.7500\\n\",\n",
        "      \"Epoch 36/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 1981269.2694 - mse: 1981268.5000 - val_loss: 1168996.8779 - val_mse: 1168996.8750\\n\",\n",
        "      \"Epoch 37/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 2857930.9917 - mse: 2857932.0000 - val_loss: 1967872.3526 - val_mse: 1967872.2500\\n\",\n",
        "      \"Epoch 38/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 2322705.1294 - mse: 2322705.5000 - val_loss: 14299594.5443 - val_mse: 14299596.0000\\n\",\n",
        "      \"Epoch 39/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 4715846.4095 - mse: 4715847.5000 - val_loss: 1852093.5321 - val_mse: 1852093.2500\\n\",\n",
        "      \"Epoch 40/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 2864054.4100 - mse: 2864054.7500 - val_loss: 836432.2193 - val_mse: 836432.4375\\n\",\n",
        "      \"Epoch 41/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 1568364.7412 - mse: 1568364.3750 - val_loss: 4361853.5706 - val_mse: 4361853.0000\\n\",\n",
        "      \"Epoch 42/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 1632025.7149 - mse: 1632026.1250 - val_loss: 1097939.3916 - val_mse: 1097939.3750\\n\",\n",
        "      \"Epoch 43/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 1458559.6601 - mse: 1458560.1250 - val_loss: 2660623.0922 - val_mse: 2660623.0000\\n\",\n",
        "      \"Epoch 44/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 1582287.6329 - mse: 1582288.3750 - val_loss: 1193838.5787 - val_mse: 1193838.5000\\n\",\n",
        "      \"Epoch 45/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 3873802.3178 - mse: 3873802.2500 - val_loss: 1098784.1124 - val_mse: 1098784.1250\\n\",\n",
        "      \"Epoch 46/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 2873203.0795 - mse: 2873203.0000 - val_loss: 5196551.7986 - val_mse: 5196551.0000\\n\",\n",
        "      \"Epoch 47/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 4743976.0634 - mse: 4743976.0000 - val_loss: 1433497.6363 - val_mse: 1433497.3750\\n\",\n",
        "      \"Epoch 48/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 4392253.7221 - mse: 4392254.0000 - val_loss: 32612177.1785 - val_mse: 32612176.0000\\n\",\n",
        "      \"Epoch 49/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 9174687.7254 - mse: 9174686.0000 - val_loss: 1672957.6137 - val_mse: 1672957.3750\\n\",\n",
        "      \"Epoch 50/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 1195647.3958 - mse: 1195647.1250 - val_loss: 1848007.4462 - val_mse: 1848007.0000\\n\",\n",
        "      \"Epoch 51/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 1597894.6775 - mse: 1597894.3750 - val_loss: 1147678.0942 - val_mse: 1147678.1250\\n\",\n",
        "      \"Epoch 52/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 1806982.0856 - mse: 1806982.3750 - val_loss: 1634108.5181 - val_mse: 1634108.6250\\n\",\n",
        "      \"Epoch 53/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 2005271.3980 - mse: 2005271.0000 - val_loss: 2823897.1612 - val_mse: 2823897.7500\\n\",\n",
        "      \"Epoch 54/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 2159620.2813 - mse: 2159620.2500 - val_loss: 1719028.7369 - val_mse: 1719028.5000\\n\",\n",
        "      \"Epoch 55/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 4921891.5085 - mse: 4921892.5000 - val_loss: 3402812.5147 - val_mse: 3402812.5000\\n\",\n",
        "      \"Epoch 56/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 2099049.2580 - mse: 2099049.0000 - val_loss: 1138593.7176 - val_mse: 1138593.8750\\n\",\n",
        "      \"Epoch 57/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 3373693.5721 - mse: 3373694.0000 - val_loss: 14401079.4950 - val_mse: 14401079.0000\\n\",\n",
        "      \"Epoch 58/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 3051282.8921 - mse: 3051282.7500 - val_loss: 817068.1912 - val_mse: 817068.1875\\n\",\n",
        "      \"Epoch 59/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 3564090.6479 - mse: 3564090.0000 - val_loss: 3046201.0085 - val_mse: 3046200.5000\\n\",\n",
        "      \"Epoch 60/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 2785511.0065 - mse: 2785511.0000 - val_loss: 4193763.5612 - val_mse: 4193763.5000\\n\",\n",
        "      \"Epoch 61/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 1614426.4830 - mse: 1614426.5000 - val_loss: 1364923.0124 - val_mse: 1364922.8750\\n\",\n",
        "      \"Epoch 62/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 1792411.9759 - mse: 1792411.7500 - val_loss: 875437.5588 - val_mse: 875437.5625\\n\",\n",
        "      \"Epoch 63/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 4039274.2920 - mse: 4039274.2500 - val_loss: 3678143.8150 - val_mse: 3678144.0000\\n\",\n",
        "      \"Epoch 64/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 3022179.4307 - mse: 3022179.5000 - val_loss: 799786.0214 - val_mse: 799785.9375\\n\",\n",
        "      \"Epoch 65/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 1948909.4646 - mse: 1948910.0000 - val_loss: 3419114.1138 - val_mse: 3419114.5000\\n\",\n",
        "      \"Epoch 66/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 2564060.0658 - mse: 2564059.0000 - val_loss: 1185971.2856 - val_mse: 1185971.1250\\n\",\n",
        "      \"Epoch 67/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 6195322.5116 - mse: 6195322.5000 - val_loss: 1794084.3446 - val_mse: 1794084.2500\\n\",\n",
        "      \"Epoch 68/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 1338029.0996 - mse: 1338028.6250 - val_loss: 1970892.6636 - val_mse: 1970892.6250\\n\",\n",
        "      \"Epoch 69/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 2203017.8647 - mse: 2203018.5000 - val_loss: 12388725.7850 - val_mse: 12388724.0000\\n\",\n",
        "      \"Epoch 70/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 1817614.8869 - mse: 1817614.7500 - val_loss: 1982097.7114 - val_mse: 1982097.5000\\n\",\n",
        "      \"Epoch 71/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 2279767.4296 - mse: 2279767.7500 - val_loss: 1004421.0417 - val_mse: 1004421.0625\\n\",\n",
        "      \"Epoch 72/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 1679896.8026 - mse: 1679896.0000 - val_loss: 1530485.0286 - val_mse: 1530484.8750\\n\",\n",
        "      \"Epoch 73/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 2ms/step - loss: 1966579.4884 - mse: 1966578.5000 - val_loss: 2357871.3720 - val_mse: 2357871.5000\\n\",\n",
        "      \"Epoch 74/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 1352281.2731 - mse: 1352281.2500 - val_loss: 719073.1831 - val_mse: 719073.1250\\n\",\n",
        "      \"Epoch 75/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 1660927.3914 - mse: 1660927.1250 - val_loss: 1633540.9482 - val_mse: 1633541.1250\\n\",\n",
        "      \"Epoch 76/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 1768205.5620 - mse: 1768206.2500 - val_loss: 1656020.2658 - val_mse: 1656020.1250\\n\",\n",
        "      \"Epoch 77/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 2266879.4457 - mse: 2266879.7500 - val_loss: 1031671.5457 - val_mse: 1031671.4375\\n\",\n",
        "      \"Epoch 78/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 1601009.6077 - mse: 1601010.3750 - val_loss: 4583446.9370 - val_mse: 4583446.0000\\n\",\n",
        "      \"Epoch 79/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 3060490.4714 - mse: 3060488.7500 - val_loss: 1035275.7533 - val_mse: 1035275.6875\\n\",\n",
        "      \"Epoch 80/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 2235310.4496 - mse: 2235309.2500 - val_loss: 3272311.7883 - val_mse: 3272311.7500\\n\",\n",
        "      \"Epoch 81/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 1652167.5323 - mse: 1652167.1250 - val_loss: 809104.2087 - val_mse: 809104.1250\\n\",\n",
        "      \"Epoch 82/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 2645250.9936 - mse: 2645251.0000 - val_loss: 4111262.9093 - val_mse: 4111263.2500\\n\",\n",
        "      \"Epoch 83/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 1693242.5105 - mse: 1693243.0000 - val_loss: 1221150.0930 - val_mse: 1221150.1250\\n\",\n",
        "      \"Epoch 84/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 2271150.4378 - mse: 2271150.7500 - val_loss: 735776.8808 - val_mse: 735776.7500\\n\",\n",
        "      \"Epoch 85/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 1293912.5995 - mse: 1293912.3750 - val_loss: 1747033.7178 - val_mse: 1747033.6250\\n\",\n",
        "      \"Epoch 86/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 3051247.9024 - mse: 3051247.5000 - val_loss: 2669215.4654 - val_mse: 2669215.0000\\n\",\n",
        "      \"Epoch 87/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 2883710.3056 - mse: 2883711.2500 - val_loss: 664998.2917 - val_mse: 664998.2500\\n\",\n",
        "      \"Epoch 88/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 1223940.3277 - mse: 1223940.1250 - val_loss: 1690747.7661 - val_mse: 1690747.5000\\n\",\n",
        "      \"Epoch 89/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 1637688.6641 - mse: 1637688.5000 - val_loss: 871721.9565 - val_mse: 871722.0625\\n\",\n",
        "      \"Epoch 90/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 1871726.8973 - mse: 1871727.3750 - val_loss: 786809.8359 - val_mse: 786809.8125\\n\",\n",
        "      \"Epoch 91/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 3525490.7870 - mse: 3525488.7500 - val_loss: 1259092.0860 - val_mse: 1259092.2500\\n\",\n",
        "      \"Epoch 92/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 4531817.2604 - mse: 4531818.0000 - val_loss: 863219.2560 - val_mse: 863219.3750\\n\",\n",
        "      \"Epoch 93/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 991653.2958 - mse: 991653.4375 - val_loss: 1188386.4006 - val_mse: 1188386.6250\\n\",\n",
        "      \"Epoch 94/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 1270158.3218 - mse: 1270158.1250 - val_loss: 633434.3314 - val_mse: 633434.4375\\n\",\n",
        "      \"Epoch 95/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 1382530.2498 - mse: 1382530.1250 - val_loss: 1314573.6194 - val_mse: 1314573.5000\\n\",\n",
        "      \"Epoch 96/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 1613063.2007 - mse: 1613063.6250 - val_loss: 992722.4749 - val_mse: 992722.3750\\n\",\n",
        "      \"Epoch 97/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 2135123.7981 - mse: 2135124.2500 - val_loss: 4087801.5133 - val_mse: 4087802.0000\\n\",\n",
        "      \"Epoch 98/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 2955573.7658 - mse: 2955572.7500 - val_loss: 1023470.5836 - val_mse: 1023470.6875\\n\",\n",
        "      \"Epoch 99/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 2881842.2950 - mse: 2881842.7500 - val_loss: 890124.7562 - val_mse: 890124.6875\\n\",\n",
        "      \"Epoch 100/100\\n\",\n",
        "      \"235/235 [==============================] - 0s 1ms/step - loss: 1263527.6351 - mse: 1263527.7500 - val_loss: 617987.8990 - val_mse: 617988.0625\\n\",\n",
        "      \"127/127 [==============================] - 0s 497us/step\\n\",\n",
        "      \"loss :  706590.5745377428\\n\",\n",
        "      \"mse :  706590.5\\n\"\n",
        "     ]\n",
        "    }\n",
        "   ],\n",
        "   \"source\": [\n",
        "    \"from keras.models import Model\\n\",\n",
        "    \"from keras.layers import Dense, Input\\n\",\n",
        "    \"\\n\",\n",
        "    \"# 모델구성\\n\",\n",
        "    \"input1 = Input(shape=(25, ))\\n\",\n",
        "    \"dense1 = Dense(64)(input1)\\n\",\n",
        "    \"dense1 = Dense(32)(dense1)\\n\",\n",
        "    \"dense1 = Dense(32)(dense1)\\n\",\n",
        "    \"output1 = Dense(32)(dense1)\\n\",\n",
        "    \"\\n\",\n",
        "    \"input2 = Input(shape=(25, ))\\n\",\n",
        "    \"dense2 = Dense(64)(input2)\\n\",\n",
        "    \"dense2 = Dense(64)(dense2)\\n\",\n",
        "    \"dense2 = Dense(64)(dense2)\\n\",\n",
        "    \"dense2 = Dense(64)(dense2)\\n\",\n",
        "    \"output2 = Dense(32)(dense2)\\n\",\n",
        "    \"\\n\",\n",
        "    \"from keras.layers.merge import concatenate\\n\",\n",
        "    \"\\n\",\n",
        "    \"merge = concatenate([output1, output2])\\n\",\n",
        "    \"output3 = Dense(1)(merge)\\n\",\n",
        "    \"\\n\",\n",
        "    \"model = Model(inputs=[input1, input2],\\n\",\n",
        "    \"              outputs = output3 )\\n\",\n",
        "    \"\\n\",\n",
        "    \"model.compile(loss='mse', optimizer='adam', metrics=['mse'])\\n\",\n",
        "    \"\\n\",\n",
        "    \"from keras.callbacks import EarlyStopping\\n\",\n",
        "    \"\\n\",\n",
        "    \"early_stopping = EarlyStopping(patience=20)\\n\",\n",
        "    \"\\n\",\n",
        "    \"model.fit([x1_train_scaled, x2_train_scaled], y1_train, validation_split=0.2, \\n\",\n",
        "    \"          verbose=1, batch_size=1, epochs=100, \\n\",\n",
        "    \"          callbacks=[early_stopping])\\n\",\n",
        "    \"\\n\",\n",
        "    \"loss, mse = model.evaluate([x1_test_scaled, x2_test_scaled], y1_test, batch_size=1)\\n\",\n",
        "    \"\\n\",\n",
        "    \"print('loss : ', loss)\\n\",\n",
        "    \"print('mse : ', mse)\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": 52,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [\n",
        "    {\n",
        "     \"name\": \"stdout\",\n",
        "     \"output_type\": \"stream\",\n",
        "     \"text\": [\n",
        "      \"종가 :  [52200] / 예측가 :  [52775.758]\\n\",\n",
        "      \"종가 :  [41450] / 예측가 :  [40969.67]\\n\",\n",
        "      \"종가 :  [49650] / 예측가 :  [50379.43]\\n\",\n",
        "      \"종가 :  [44800] / 예측가 :  [46991.727]\\n\",\n",
        "      \"종가 :  [49500] / 예측가 :  [49205.47]\\n\"\n",
        "     ]\n",
        "    }\n",
        "   ],\n",
        "   \"source\": [\n",
        "    \"y1_pred = model.predict([x1_test_scaled, x2_test_scaled])\\n\",\n",
        "    \"\\n\",\n",
        "    \"for i in range(5):\\n\",\n",
        "    \"    print('종가 : ', y1_test[i], '/ 예측가 : ', y1_pred[i])\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": 53,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [\n",
        "    {\n",
        "     \"name\": \"stdout\",\n",
        "     \"output_type\": \"stream\",\n",
        "     \"text\": [\n",
        "      \"(294, 5, 5)\\n\",\n",
        "      \"(127, 5, 5)\\n\",\n",
        "      \"(294, 1)\\n\",\n",
        "      \"(127, 1)\\n\"\n",
        "     ]\n",
        "    }\n",
        "   ],\n",
        "   \"source\": [\n",
        "    \"# sam05_ensemble.py\\n\",\n",
        "    \"\\n\",\n",
        "    \"\\n\",\n",
        "    \"# 데이터 셋 나누기\\n\",\n",
        "    \"from sklearn.model_selection import train_test_split\\n\",\n",
        "    \"# from sklearn.model_selection import cross_val_score\\n\",\n",
        "    \"x1_train, x1_test, y1_train, y1_test = train_test_split(\\n\",\n",
        "    \"    x1, y1, random_state=1, test_size = 0.3)\\n\",\n",
        "    \"x2_train, x2_test, y2_train, y2_test = train_test_split(\\n\",\n",
        "    \"    x2, y2, random_state=2, test_size = 0.3)\\n\",\n",
        "    \"\\n\",\n",
        "    \"print(x2_train.shape)\\n\",\n",
        "    \"print(x2_test.shape)\\n\",\n",
        "    \"print(y2_train.shape)\\n\",\n",
        "    \"print(y2_test.shape)\\n\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": 54,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [\n",
        "    {\n",
        "     \"name\": \"stdout\",\n",
        "     \"output_type\": \"stream\",\n",
        "     \"text\": [\n",
        "      \"(294, 25)\\n\",\n",
        "      \"(127, 25)\\n\"\n",
        "     ]\n",
        "    }\n",
        "   ],\n",
        "   \"source\": [\n",
        "    \"x1_train = np.reshape(x1_train,\\n\",\n",
        "    \"    (x1_train.shape[0], x1_train.shape[1] * x1_train.shape[2]))\\n\",\n",
        "    \"x1_test = np.reshape(x1_test,\\n\",\n",
        "    \"    (x1_test.shape[0], x1_test.shape[1] * x1_test.shape[2]))\\n\",\n",
        "    \"x2_train = np.reshape(x2_train,\\n\",\n",
        "    \"    (x2_train.shape[0], x2_train.shape[1] * x2_train.shape[2]))\\n\",\n",
        "    \"x2_test = np.reshape(x2_test,\\n\",\n",
        "    \"    (x2_test.shape[0], x2_test.shape[1] * x2_test.shape[2]))\\n\",\n",
        "    \"\\n\",\n",
        "    \"print(x2_train.shape)\\n\",\n",
        "    \"print(x2_test.shape)\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": 55,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [\n",
        "    {\n",
        "     \"name\": \"stdout\",\n",
        "     \"output_type\": \"stream\",\n",
        "     \"text\": [\n",
        "      \"[ 0.58843327  0.533494    0.64328272  0.57661608 -0.49913908  0.60996636\\n\",\n",
        "      \"  0.54270686  0.36243605  0.29103458  0.25643568  0.37564145  0.30537859\\n\",\n",
        "      \"  0.39643403  0.3025121  -0.92269429  0.30427131  0.31328749  0.31314525\\n\",\n",
        "      \"  0.30057603 -1.24269186  0.29584292  0.2699222   0.37789775  0.34761784\\n\",\n",
        "      \" -0.73601691]\\n\"\n",
        "     ]\n",
        "    }\n",
        "   ],\n",
        "   \"source\": [\n",
        "    \"#### 데이터 전처리 #####\\n\",\n",
        "    \"\\n\",\n",
        "    \"from sklearn.preprocessing import StandardScaler\\n\",\n",
        "    \"\\n\",\n",
        "    \"scaler1 = StandardScaler()\\n\",\n",
        "    \"scaler1.fit(x1_train)\\n\",\n",
        "    \"x1_train_scaled = scaler1.transform(x1_train)\\n\",\n",
        "    \"x1_test_scaled = scaler1.transform(x1_test)\\n\",\n",
        "    \"scaler2 = StandardScaler()\\n\",\n",
        "    \"scaler2.fit(x2_train)\\n\",\n",
        "    \"x2_train_scaled = scaler2.transform(x2_train)\\n\",\n",
        "    \"x2_test_scaled = scaler2.transform(x2_test)\\n\",\n",
        "    \"\\n\",\n",
        "    \"print(x2_train_scaled[0, :])\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": 56,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [\n",
        "    {\n",
        "     \"name\": \"stdout\",\n",
        "     \"output_type\": \"stream\",\n",
        "     \"text\": [\n",
        "      \"(294, 5, 5)\\n\",\n",
        "      \"(127, 5, 5)\\n\"\n",
        "     ]\n",
        "    }\n",
        "   ],\n",
        "   \"source\": [\n",
        "    \"x1_train_scaled = np.reshape(x1_train_scaled,\\n\",\n",
        "    \"    (x1_train_scaled.shape[0], 5, 5))\\n\",\n",
        "    \"x1_test_scaled = np.reshape(x1_test_scaled,\\n\",\n",
        "    \"    (x1_test_scaled.shape[0], 5, 5))\\n\",\n",
        "    \"x2_train_scaled = np.reshape(x2_train_scaled,\\n\",\n",
        "    \"    (x2_train_scaled.shape[0], 5, 5))\\n\",\n",
        "    \"x2_test_scaled = np.reshape(x2_test_scaled,\\n\",\n",
        "    \"    (x2_test_scaled.shape[0], 5, 5))\\n\",\n",
        "    \"\\n\",\n",
        "    \"print(x2_train_scaled.shape)\\n\",\n",
        "    \"print(x2_test_scaled.shape)\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": 57,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [\n",
        "    {\n",
        "     \"name\": \"stdout\",\n",
        "     \"output_type\": \"stream\",\n",
        "     \"text\": [\n",
        "      \"Train on 235 samples, validate on 59 samples\\n\",\n",
        "      \"Epoch 1/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 5ms/step - loss: 859312801.0851 - mse: 859313344.0000 - val_loss: 19841819.1208 - val_mse: 19841818.0000\\n\",\n",
        "      \"Epoch 2/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 3ms/step - loss: 14296099.6979 - mse: 14296095.0000 - val_loss: 8774015.9168 - val_mse: 8774014.0000\\n\",\n",
        "      \"Epoch 3/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 6022971.1096 - mse: 6022971.0000 - val_loss: 3639014.4097 - val_mse: 3639013.7500\\n\",\n",
        "      \"Epoch 4/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 2985054.1389 - mse: 2985054.5000 - val_loss: 1898877.6551 - val_mse: 1898877.8750\\n\",\n",
        "      \"Epoch 5/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 1799231.0379 - mse: 1799230.5000 - val_loss: 1086478.3736 - val_mse: 1086478.1250\\n\",\n",
        "      \"Epoch 6/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 1134748.5393 - mse: 1134748.7500 - val_loss: 1483043.8813 - val_mse: 1483043.5000\\n\",\n",
        "      \"Epoch 7/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 1415149.2953 - mse: 1415149.3750 - val_loss: 769895.8172 - val_mse: 769895.8125\\n\",\n",
        "      \"Epoch 8/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 1091878.9664 - mse: 1091879.1250 - val_loss: 1293802.9140 - val_mse: 1293802.7500\\n\",\n",
        "      \"Epoch 9/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 1091016.3403 - mse: 1091016.8750 - val_loss: 694108.2674 - val_mse: 694108.5000\\n\",\n",
        "      \"Epoch 10/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 885979.7312 - mse: 885979.5625 - val_loss: 747735.7037 - val_mse: 747735.8125\\n\",\n",
        "      \"Epoch 11/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 876112.1557 - mse: 876112.2500 - val_loss: 2023216.4859 - val_mse: 2023216.3750\\n\",\n",
        "      \"Epoch 12/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 1037723.0724 - mse: 1037722.8125 - val_loss: 1363447.5458 - val_mse: 1363447.5000\\n\",\n",
        "      \"Epoch 13/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 1029476.7551 - mse: 1029476.7500 - val_loss: 635433.6282 - val_mse: 635433.7500\\n\",\n",
        "      \"Epoch 14/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 1314519.9762 - mse: 1314519.7500 - val_loss: 1100303.5678 - val_mse: 1100303.5000\\n\",\n",
        "      \"Epoch 15/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 912880.6319 - mse: 912880.5000 - val_loss: 2460001.4005 - val_mse: 2460002.0000\\n\",\n",
        "      \"Epoch 16/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 919355.2160 - mse: 919355.2500 - val_loss: 3025442.3780 - val_mse: 3025442.7500\\n\",\n",
        "      \"Epoch 17/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 819735.1214 - mse: 819734.8750 - val_loss: 1071146.1558 - val_mse: 1071146.3750\\n\",\n",
        "      \"Epoch 18/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 1040075.5601 - mse: 1040075.5000 - val_loss: 1259860.9395 - val_mse: 1259860.7500\\n\",\n",
        "      \"Epoch 19/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 762555.3339 - mse: 762555.3750 - val_loss: 2498865.6029 - val_mse: 2498865.7500\\n\",\n",
        "      \"Epoch 20/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 3ms/step - loss: 1267797.0405 - mse: 1267797.2500 - val_loss: 603059.4180 - val_mse: 603059.4375\\n\",\n",
        "      \"Epoch 21/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 3ms/step - loss: 833323.0838 - mse: 833323.1875 - val_loss: 644633.9578 - val_mse: 644633.8750\\n\",\n",
        "      \"Epoch 22/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 3ms/step - loss: 991194.2951 - mse: 991194.5000 - val_loss: 619630.3147 - val_mse: 619630.1875\\n\",\n",
        "      \"Epoch 23/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 3ms/step - loss: 804690.3280 - mse: 804690.0625 - val_loss: 991478.5797 - val_mse: 991478.5625\\n\",\n",
        "      \"Epoch 24/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 3ms/step - loss: 950169.3298 - mse: 950169.6875 - val_loss: 843051.2795 - val_mse: 843051.3750\\n\",\n",
        "      \"Epoch 25/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 946428.4008 - mse: 946428.6250 - val_loss: 1952225.7548 - val_mse: 1952225.6250\\n\",\n",
        "      \"Epoch 26/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 914526.6203 - mse: 914526.8125 - val_loss: 756221.8237 - val_mse: 756221.7500\\n\",\n",
        "      \"Epoch 27/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 906533.1432 - mse: 906533.3125 - val_loss: 588618.8610 - val_mse: 588618.7500\\n\",\n",
        "      \"Epoch 28/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 993448.8230 - mse: 993448.6250 - val_loss: 725611.0383 - val_mse: 725611.1250\\n\",\n",
        "      \"Epoch 29/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 1358870.1154 - mse: 1358870.2500 - val_loss: 1282970.8429 - val_mse: 1282970.7500\\n\",\n",
        "      \"Epoch 30/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 918779.9798 - mse: 918780.0625 - val_loss: 1035702.3427 - val_mse: 1035702.5625\\n\",\n",
        "      \"Epoch 31/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 812238.8368 - mse: 812238.6875 - val_loss: 580138.5145 - val_mse: 580138.4375\\n\",\n",
        "      \"Epoch 32/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 913954.3885 - mse: 913954.0625 - val_loss: 684331.3059 - val_mse: 684331.2500\\n\",\n",
        "      \"Epoch 33/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 882438.4000 - mse: 882438.3125 - val_loss: 681564.5914 - val_mse: 681564.5625\\n\",\n",
        "      \"Epoch 34/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 1122168.5864 - mse: 1122168.6250 - val_loss: 724440.2586 - val_mse: 724440.3125\\n\",\n",
        "      \"Epoch 35/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 796637.1766 - mse: 796637.1250 - val_loss: 710610.0335 - val_mse: 710610.1875\\n\",\n",
        "      \"Epoch 36/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 963113.9163 - mse: 963113.8750 - val_loss: 1425653.0730 - val_mse: 1425653.1250\\n\",\n",
        "      \"Epoch 37/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 947578.1940 - mse: 947577.8750 - val_loss: 1172134.3455 - val_mse: 1172134.3750\\n\",\n",
        "      \"Epoch 38/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 869197.8658 - mse: 869197.9375 - val_loss: 834399.7222 - val_mse: 834399.5625\\n\",\n",
        "      \"Epoch 39/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 1168858.0469 - mse: 1168858.2500 - val_loss: 675405.0477 - val_mse: 675405.0625\\n\",\n",
        "      \"Epoch 40/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 895120.0500 - mse: 895120.0625 - val_loss: 609779.2091 - val_mse: 609779.1875\\n\",\n",
        "      \"Epoch 41/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 874775.2114 - mse: 874775.1875 - val_loss: 998123.4067 - val_mse: 998123.5000\\n\",\n",
        "      \"Epoch 42/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 1057792.7889 - mse: 1057792.8750 - val_loss: 1811956.3025 - val_mse: 1811956.3750\\n\",\n",
        "      \"Epoch 43/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 3ms/step - loss: 1263650.6455 - mse: 1263650.1250 - val_loss: 572714.6095 - val_mse: 572714.5625\\n\",\n",
        "      \"Epoch 44/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 899188.6521 - mse: 899189.0000 - val_loss: 620787.1996 - val_mse: 620787.2500\\n\",\n",
        "      \"Epoch 45/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 3ms/step - loss: 723892.4375 - mse: 723892.3125 - val_loss: 1179360.5425 - val_mse: 1179360.6250\\n\",\n",
        "      \"Epoch 46/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 3ms/step - loss: 673329.5912 - mse: 673329.3750 - val_loss: 1347131.4064 - val_mse: 1347131.6250\\n\",\n",
        "      \"Epoch 47/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 1052221.7855 - mse: 1052221.6250 - val_loss: 598738.9839 - val_mse: 598738.9375\\n\",\n",
        "      \"Epoch 48/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 938945.4944 - mse: 938945.2500 - val_loss: 721524.3178 - val_mse: 721524.1250\\n\",\n",
        "      \"Epoch 49/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 1009535.6154 - mse: 1009535.8125 - val_loss: 640638.6990 - val_mse: 640638.6875\\n\",\n",
        "      \"Epoch 50/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 750444.1442 - mse: 750444.1250 - val_loss: 582105.6440 - val_mse: 582105.6250\\n\",\n",
        "      \"Epoch 51/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 1024040.0620 - mse: 1024040.5000 - val_loss: 697957.7801 - val_mse: 697957.8125\\n\",\n",
        "      \"Epoch 52/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 705565.0548 - mse: 705565.1875 - val_loss: 913358.9803 - val_mse: 913359.0625\\n\",\n",
        "      \"Epoch 53/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 974172.7550 - mse: 974172.5000 - val_loss: 740576.9120 - val_mse: 740576.9375\\n\",\n",
        "      \"Epoch 54/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 786934.7656 - mse: 786934.6250 - val_loss: 639072.8176 - val_mse: 639072.6875\\n\",\n",
        "      \"Epoch 55/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 785025.3703 - mse: 785025.3125 - val_loss: 736837.8206 - val_mse: 736837.8750\\n\",\n",
        "      \"Epoch 56/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 901218.7683 - mse: 901218.6250 - val_loss: 1483465.8553 - val_mse: 1483465.8750\\n\",\n",
        "      \"Epoch 57/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 1026226.7522 - mse: 1026226.9375 - val_loss: 810040.7293 - val_mse: 810040.8125\\n\",\n",
        "      \"Epoch 58/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 703663.9565 - mse: 703664.0625 - val_loss: 866626.7100 - val_mse: 866626.6875\\n\",\n",
        "      \"Epoch 59/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 986523.5935 - mse: 986523.3750 - val_loss: 1780759.9647 - val_mse: 1780760.0000\\n\",\n",
        "      \"Epoch 60/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 843703.8985 - mse: 843703.7500 - val_loss: 1230980.9982 - val_mse: 1230981.0000\\n\",\n",
        "      \"Epoch 61/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 797915.4033 - mse: 797915.4375 - val_loss: 618245.1171 - val_mse: 618245.2500\\n\",\n",
        "      \"Epoch 62/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 837316.6281 - mse: 837316.6250 - val_loss: 1066455.0751 - val_mse: 1066455.0000\\n\",\n",
        "      \"Epoch 63/100\\n\",\n",
        "      \"235/235 [==============================] - 1s 2ms/step - loss: 719805.3279 - mse: 719805.5625 - val_loss: 2003439.6723 - val_mse: 2003439.6250\\n\",\n",
        "      \"127/127 [==============================] - 0s 799us/step\\n\",\n",
        "      \"loss :  2782827.537228562\\n\",\n",
        "      \"mse :  2782827.25\\n\"\n",
        "     ]\n",
        "    }\n",
        "   ],\n",
        "   \"source\": [\n",
        "    \"from keras.models import Model\\n\",\n",
        "    \"from keras.layers import Dense, Input, LSTM\\n\",\n",
        "    \"\\n\",\n",
        "    \"# 모델구성\\n\",\n",
        "    \"input1 = Input(shape=(5, 5))\\n\",\n",
        "    \"dense1 = LSTM(64)(input1)\\n\",\n",
        "    \"dense1 = Dense(32)(dense1)\\n\",\n",
        "    \"dense1 = Dense(32)(dense1)\\n\",\n",
        "    \"output1 = Dense(32)(dense1)\\n\",\n",
        "    \"\\n\",\n",
        "    \"input2 = Input(shape=(5, 5))\\n\",\n",
        "    \"dense2 = LSTM(64)(input2)\\n\",\n",
        "    \"dense2 = Dense(64)(dense2)\\n\",\n",
        "    \"dense2 = Dense(64)(dense2)\\n\",\n",
        "    \"dense2 = Dense(64)(dense2)\\n\",\n",
        "    \"output2 = Dense(32)(dense2)\\n\",\n",
        "    \"\\n\",\n",
        "    \"from keras.layers.merge import concatenate\\n\",\n",
        "    \"\\n\",\n",
        "    \"merge = concatenate([output1, output2])\\n\",\n",
        "    \"output3 = Dense(1)(merge)\\n\",\n",
        "    \"\\n\",\n",
        "    \"model = Model(inputs=[input1, input2],\\n\",\n",
        "    \"              outputs = output3 )\\n\",\n",
        "    \"\\n\",\n",
        "    \"\\n\",\n",
        "    \"model.compile(loss='mse', optimizer='adam', metrics=['mse'])\\n\",\n",
        "    \"\\n\",\n",
        "    \"from keras.callbacks import EarlyStopping\\n\",\n",
        "    \"\\n\",\n",
        "    \"early_stopping = EarlyStopping(patience=20)\\n\",\n",
        "    \"model.fit([x1_train_scaled, x2_train_scaled], y1_train, validation_split=0.2, \\n\",\n",
        "    \"          verbose=1, batch_size=1, epochs=100, \\n\",\n",
        "    \"          callbacks=[early_stopping])\\n\",\n",
        "    \"\\n\",\n",
        "    \"loss, mse = model.evaluate([x1_test_scaled, x2_test_scaled], y1_test, batch_size=1)\\n\",\n",
        "    \"print('loss : ', loss)\\n\",\n",
        "    \"print('mse : ', mse)\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": 58,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [\n",
        "    {\n",
        "     \"name\": \"stdout\",\n",
        "     \"output_type\": \"stream\",\n",
        "     \"text\": [\n",
        "      \"종가 :  [52200] / 예측가 :  [51000.086]\\n\",\n",
        "      \"종가 :  [41450] / 예측가 :  [39461.91]\\n\",\n",
        "      \"종가 :  [49650] / 예측가 :  [49345.016]\\n\",\n",
        "      \"종가 :  [44800] / 예측가 :  [45215.414]\\n\",\n",
        "      \"종가 :  [49500] / 예측가 :  [48439.484]\\n\"\n",
        "     ]\n",
        "    }\n",
        "   ],\n",
        "   \"source\": [\n",
        "    \"y1_pred = model.predict([x1_test_scaled, x2_test_scaled])\\n\",\n",
        "    \"\\n\",\n",
        "    \"for i in range(5):\\n\",\n",
        "    \"    print('종가 : ', y1_test[i], '/ 예측가 : ', y1_pred[i])\"\n",
        "   ]\n",
        "  }\n",
        " ],\n",
        " \"metadata\": {\n",
        "  \"kernelspec\": {\n",
        "   \"display_name\": \"Python 3\",\n",
        "   \"language\": \"python\",\n",
        "   \"name\": \"python3\"\n",
        "  },\n",
        "  \"language_info\": {\n",
        "   \"codemirror_mode\": {\n",
        "    \"name\": \"ipython\",\n",
        "    \"version\": 3\n",
        "   },\n",
        "   \"file_extension\": \".py\",\n",
        "   \"mimetype\": \"text/x-python\",\n",
        "   \"name\": \"python\",\n",
        "   \"nbconvert_exporter\": \"python\",\n",
        "   \"pygments_lexer\": \"ipython3\",\n",
        "   \"version\": \"3.7.5\"\n",
        "  }\n",
        " },\n",
        " \"nbformat\": 4,\n",
        " \"nbformat_minor\": 2\n",
        "}\n"
      ]
    }
  ]
}